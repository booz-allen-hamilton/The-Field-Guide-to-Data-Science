<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
### 数据科学战地指南
Booz Allen Hamilton 公司的 The Field Guide to Data Science 中文翻译
<br/>
<br/>

> 翻译参与者： 张夏天  张贻红 阎志涛 黄洋成 叶杰生 刘海军 何海洋 韩森饶 冯博 王鹏 马子俊 田永军 王小辉 @ TalkingData

<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>




## 前言

> 数据科学影响着我们生活的方方面面，无论我们是去看医生，开车，坐飞机或是购物，数据科学都在潜移默化的改变着我们与这个世界之间的互动方式。

__数字符号正逐渐地计量和记录着我们的世界__。数字符号正逐渐地计量和记录着我们的世界。我们整个生命从出生到死亡，都被编入了数字王国的目录中。这些数据来自各种各样不同的源，比如交通工具、水下摄像机、社交媒体中发布的图片等等，这些数据将促使我们成为人类有史以来最为充满发现的时代。通过数据科学，我们解锁着数据背后隐藏的秘密。这些发现将会永远改变我们在这个世界的生活方式与互动方式。

这些变化正对人类产生着深远的影响。技术的不断进步让我们将自己推入这个充满发现的时代。数据科学已经成为我们下一个革命性飞跃背后的催化力量。我们自身的进化现在已经和计算机密不可分，我们的生活方式和重要生存技能，都直接依赖于数据科学为我们的带来的种种功能。

随着进入新时代，我们要比之前更清晰地认识到，企业必须针对这些变化和风险作出调整。从如何影响零售市场，到制定公共健康和安全政策，或解决社会动荡的方方面面，所有类型的组织都要通过数据科学来产生价值。数据是我们新的货币，而数据科学就是发掘它的手法

数据科学利用我们的好奇心和对技术的追求来解决人类面临的严峻挑战，这最有效和深刻的方法。应用数据科学方法来应对这些挑战有一种不断增长的力量，其重要性和责任是难以想象的。我们的偏见和假设都可能对商业，国家安全和日常生活都有着深刻的影响。一批新的从业人员和领导者们将在这片新领域航行，数据科学家是我们这个征途上的导游，他们正创造一种全新的方式来思考数据和我们周围的世界。
<br/>

## 战地指南的故事

__几年前，我们创作了《The Field Guide to Data Science》这本书，因为我们想要帮助各类型和规模的组织__。有不计其数的企业和学术机构描述了数据科学是什么和为什么要关心数据科学，但却很少有人解释如何像资源一样来利用数据。我们发现两年前出版第一版战地指南时的情形，和现在并没有什么区别。

在博思艾伦咨询公司，我们组建了一个业界领先的数据科学家团队。在解决不计其数的客户分析需求的过程中，我们已经揭开数据科学基因的面纱。很多人致力于数据科学的某一方面。我们相信我们可以对数据科学的概念模型、情报、处理流程和文化等方面提供一个更广阔的视角，包括是什么，为什么，谁，怎么样。通常有着强大数据科学团队的公司往往致力于解决某一类问题，比如社会网络分析图算法和网上购物推荐系统就是两个典型的例子。博思艾伦并不是这样，我们担任咨询的角色时，可以支持在不同领域的客户，包括各种政府和商业客户，这让我们成为了唯一一个掌握数据科学本质的公司。

我们创作《The Field Guide to Data Science》的目标是总结学到的东西并把它们广泛分享。从我们发布第一版战地指南后，数据科学领域还在不断进步发展，因此我们决定结合一些两年来比较重要的新概念发布第二版。我们还在技术上增加了深度和丰富性，我们相信实践者们会觉得它们很有用。

希望我们的努力可以持续推进数据科学的科学和艺术。

<br/>

## 我们都是这个故事的作者

__我们认为数据科学是一个团队的活动__。数据科学战地指南是博思艾伦咨询公司对于复杂和有些神秘的数据科学领域的一些观点。我们的理解并不代表就是全部的数据科学。我们也并不能完全跟上这个领域发展的步伐，我们现在的观点已经落后于这个领域的快速发展。因此，我们把这本领域指南推向世界，随着科技，专业知识和技术的不断提高，我们会动态更新这本书。

感谢所有那些发邮件提供给我们想法的人，特别是100多位一直以来关注，使用我们github仓库的人。我们真的很看重这个群体对我们的贡献，正是因为我们一起去努力才推动了数据科学的进步。这就是为什么我们在数据科学战地指南第二版的作者中包括了除博思艾伦咨询公司之外的人。

如果你发现这本指南有用，优雅，或是有缺陷，我们非常鼓励您来贡献专业知识，包括：

- 你学过的案例
- 期刊文章中给你启发的论文
- 你喜欢的算法和技术
- 你对于其他朋友的贡献的想法和评论

> 将你们的想法和观点邮件发至data_science@bah.com；或者通过我们的github仓库来提交请求。加入我们的讨论，和我们一起开启这段旅程。告诉我们你知道的世界。来成为这个故事的作者吧！

<br/>

## 致谢

我们想要向那些帮助这本数据科学战地指南获得如此成功的人致以最真诚的感谢。

感谢那些从我们网站上下载了电子版的15000多位朋友。感谢那些与我们在战地指南的github页面上交流联系的100多位朋友。我们对我们的作品在数据科学的群体中如此受欢迎，感到非常地受宠若惊。 

感谢所有用我们的战地指南的读者。我们非常高兴我们的作品有如此大的影响，从构建技术方法，到服务于建立数据科学在政府和商业机构的定义和角色基础。

感谢那些用我们的战地指南作为教材的老师和学者。我们非常感谢你们信任这本书，以这本书作为入门引领学生了解数据科学。我们对于这本书能帮助到下一代数据科学家感到非常荣幸。

感谢那些与我们分享成功故事，给予我们反馈和鼓励的领导。我们对这本书能帮助到如此多的组织和机构开启他们的数据科学旅程感到非常激动，这些组织和机构涉及能源、生命科学和零售。

我们真诚地希望所有读者能继续从这本书中找到价值。值此第二版本出版的时候，一起与我们分享快乐。请继续跟我们一起参与这个讨论和这趟旅程。

<br/>

## 目录

1. 前言
2. 战地指南的故事
3. 我们都是这个故事的作者
4. 致谢
5. 目录
6. 认识你的导师
7. 指南精炼版——数据科学的核心概念
8. 起步基础——数据科学导论	
	- 我们所说的数据科学是什么？
	- 数据科学实际上是如何工作的？
	- 创建数据科学能力需要什么?
9. 拆掉训练轮——数据科学从业者指南
	- 指导原则
	- 推理的重要性
	- 数据科学的元器件
	- 分形分析模型
	- 分析方法选择流程
	- 分析方法选择指南
	- 分析方法明细表
10. 战壕里的生活——在齐脖深的数据里航行
	- 深入机器学习
	- 特征工程
	- 特征选择
	- 模型融合
	- 数据准确性
	- 领域知识的应用
	- 维度诅咒
	- 模型验证
6. 全部放到一起——我们的案例研究
	- 精简药物审查
	- 减少航班延误
	- 让疫苗更安全
	- 预测大屠杀发生的相对风险以组织阻止未来的暴行
	- 预测客户的反应
7. 结束时间
	- 数据科学的未来
	- 结束语
	- 参考文献


## 认识你的导师

- Fred Blackburn ([@boozallen](https://twitter.com/BoozAllen)):

	Data Science是一个快速进化的领域，非常高兴能和大家一起成长。

- Josh Sullivan ([@joshdsullivan](https://twitter.com/joshdsullivan)):

	Data Science团队的工作每天都在展现人类好奇心和发现力的神奇力量。不要抗拒融合艺术和科学来发展你自己的数据分析观，这两者的结合会是一个强有力的工具。

- Peter Guerra9 ([@petrguerra](https://twitter.com/petrguerra)):

	Data Science是有关于艺术、数学、代码、汗水和泪水的奇妙组合。它能让你体验直冲云霄转瞬间又跌落深渊；但这是我们理解和描述"为什么"的必由之路。

- Angela Zutavern ([@angelazutavern](https://twitter.com/angelazutavern)):

	Data Science就是提出更大的问题，预见未来的可能性，以及创造期望的结果。

- Steve Ascaravage ([@sescarav](https://twitter.com/sescarav)):

	将时间和精力更多投入那些很难拼装起来的数据；如果它还不存在，就想办法把它拼起来。

- Ezmeralda Khalil ([@ezmeraldakhalil](https://twitter.com/ezmeraldakhalil)):

	Data Science重在执行。

- Steven Mills ([@stevndmills](https://twitter.com/stevndmills)):

	Data Science真真切切可以改变世界。

- Alex Cosmas ([@boozallen](https://twitter.com/boozallen)):

	数据科学家应该寻找真理而不是描述事实。

- Brian Keller ([@boozallen](https://twitter.com/boozallen)):

	勇气和决心而不是天赋会带你走的更远。

- Stephanie Beben ([@BoozAllen](https://twitter.com/BoozAllen)):

	带着深深的好奇心和适度的怀疑来开始一个数据项目挑战。

- Kirk Borne ([@KirkDBorne](https://twitter.com/KirkDBorne)):

	关注数据价值，而不是数据量。 

- Drew Ferris ([@drewfarris](https://twitter.com/drewfarris)):

	保持游戏的心态；平日多玩玩工具，玩玩数据，玩玩算法；说不定你就可以发现一些东西，可以解决下一个让你揪心的问题。
	
- Paul Yacci ([@paulyacci](https://twitter.com/paulyacci)):

	在数据的丛林里，即不要忘了树，也不要忘了森林。
	
- Charles Glover ([@MindAfterMath](https://twitter.com/MindAfterMath)):

	数据科学之美，在于通过数据和算法的游戏，来满足关于一些重要问题的好奇心。

- Michael Kim ([@BoozAllen](https://twitter.com/BoozAllen)):

	Data Science 既是艺术，也是科学。 

- Stephanie Rivera ([@BoozAllen](https://twitter.com/BoozAllen)):

	Data Science 就像攀岩，专注会让你不断得到提升，坚持会带领你到顶峰。
	
- Aaron Sander ([@ajsander](https://twitter.com/ajsander)):

	Data Science 会让企业文化变得更像 open-source 环境：越开放，会有越多的合作，也就走的越快。
	
- Will Cukierski __kaggle__ [](https://twitter.com/kaggle)

	树林里有两条分岔路，我会选择‘负梯度’的那条，而后柳暗花明。 
	
- Mark Herman ([@cloudEBITDA](https://twitter.com/cloudEBITDA)):

	用 『因此』 来结束每天都分析工作。 

- Ed Kohlwey ([@ekohlwey](https://twitter.com/ekohlwey)):

	从分析你身边的所有东西开始，然后变得依赖数据驱动。

- Armen Kherlopian ([@akherlopian](https://twitter.com/akherlopian)):

	数据科学家必须持续不停的在含混不清的情况下寻求真相，因此必须基于精确和洞察。

我们要感谢如下这些人对这本书的贡献:

- Tim Andrews
- Mike Delurey
- Greg Dupier
- Jason Escaravage
- Christine Fantaskey
- Juergen Klenk
- Dan Liebermann, Mark
- Rockley
- Katie Wilks

 <br/>

## 指南精炼版 - 数据科学的核心概念

###精炼版
__数据科学是将数据转化为行动的艺术__

数据科学归根结底是tradecraft。tradecraft是人和计算机一起工作将数据转化成洞察的流程，工具和技术。

__数据科学tradecraft创造数据产品__

数据产品提供可执行的信息，但是不会将底层的数据和分析暴露给决策者 （例如， 金融工具买卖策略，提高产品收益的行动组合， 或者是改进产品营销的步骤）

__数据科学支持和鼓励在论断式(基于假说的)推理和诱导式（基于模式的）推理之间切换。__

这是对传统分析方法的本质性改变。诱导式推理和探索性的数据分析提供了形成和改进假设和发现新的分析路径。 
模型的现实性不再要求是静态的。 模型持续的被测试，更新和改进，直到更好的模型被发现。

__数据科学能力是可以通过花时间来建设的__

组织的数据科学能力要通过以下几个阶段逐步走向成熟 ： 收集，描述，发现，预测和建议。这个过程就是从被数据淹没到达成全面成熟的数据科学的过程。在每个阶段，他们可以使用更宽的分析能力来处理复杂度比上一阶段更高的分析目标。然而，组织并不需要达到最高的数据科学成熟度才能取得成功。在每个一个阶段都能取得显著的收益。

__数据科学是一种不同的团体性运动。__

数据科学团队需要具有对整个组织的宽广视野。其领导者必须是主要的倡导者，他们邀请利益相关方一起找出最艰难的挑战，找到数据，将各种分离的业务碎片拼凑到一起，并且能够获得广泛的支持。

<br/>

## 起步基础——数据科学导论

__如果你没有听说过数据科学，那么你已经落后于时代了。只是将商业智能团队改名为数据科学团队不是解决方案。__

### 从这里开始：我们所说的数据科学是什么？

描述数据科学就像尝试描述日落——应该很简单，但是找到合适的词来描述却是很难的。

#### 数据科学定义

数据科学是将数据转变成行动的艺术。它是通过创建数据产品，从而提供可执行的信息而不向决策者暴露底层的数据或者分析过程(例如， 金融工具的买卖策略， 提高产品收益的行动组合， 或者是改进产品营销的步骤 )。

实施数据科学需要从多样的数据源中提炼及时并可执行的信息来驱动数据产品。数据产品的例子包括如下问题的答案：“我的哪个产品需要加大广告宣传的力度来提升利润? 我怎么样才能在改进合规程序的同时削减成本？ 改变什么制造程序可以让我制作出更好产品？” 回答这些问题的关键点是： 理解你所拥有的数据和这些数据诱导性的告诉你的东西。

---

> 数据产品
> 
> 数据产品提供可执行的信息，但是不会将底层的数据和分析暴露给决策者 。例如：
> 
> - 电影推荐
> - 天气预报
> - 证券市场预测
> - 产品流程改进
> - 健康诊断
> - 流感趋势预测
> - 定向广告

---

请阅读附加背景知识：

数据科学这个术语是在1960-1980年代期间出现在计算机科学文献中的。然而直到1990年代后期，本书所讲的数据科学才开始在统计学和数据挖掘社区里出现（例如，文献[2]和[3]）。2001年数据科学第一次作为一个独立的学科被提出[4]。从此之后，出现了大量的文章倡导这一学科。数据科学甚至被称为21世纪最性感的职业[5].

#### 是什么让数据科学不同
 
数据科学支持和鼓励在论断式(基于假说的)推理和诱导式（基于模式的）推理之间切换。这是对传统分析方法的本质性改变。诱导式推理和探索性的数据分析提供了形成和改进假设和发现新的分析路径。事实上，要达成发现有重要意义的洞察这一数据科学的重要标志，你必须掌握tradecraft以及论断式与诱导式的相互作用。依靠积极的组合论断式和诱导式的推理能力，数据科学创造了一个现实模型不再需要是静态以及基于经验的环境。反而，模型持续被测试，更新和改进直到更好的模型被找到。Data Science Tradecraft推理类型和他们的角色。

推理类型…

<table class="table table table-bordered table-striped table-condensed">
<tr>
<td>论断式推理</td>
<td>诱导式推理</td>
</tr>
<tr>
<td>通常与“形式逻辑”相关</td>
<td>通常被认为是“非形式逻辑”， 或者“日常争论”</td>
</tr>
<tr>
<td>涉及到从已知的前提，或者假设为真的前提，获得确定的结论。</td>
<td>涉及到基于概率性推理的非确定性推论</td>
</tr>
<tr>
<td>其结论是确定，必然的，和不可避免的。</td>
<td>其结论是大概的，合理的，有道理的，和可信的</td>
</tr>
</table>

…以及他们在数据科学TRADECRAFT中的角色

<table class="table table table-bordered table-striped table-condensed">
<tr>
<td>论断式推理</td>
<td>诱导式推理</td>
</tr>
<tr>
<td>将与关系和基础模型相关的假设形式化</td>
<td>探索式数据分析可以发现或者优化假设</td>
</tr>
<tr>
<td>用数据进行实验来测试假设和模型</td>
<td>从数据中发现新的关系，洞察和分析路径。</td>
</tr>
</table>

数据科学和传统分析方法之间的不同并不止于在论断式和诱导式推理间无缝的切换。数据科学提供了一种显著不同于商业智能能力的观点。然而，在一个组从事织内，数据科学并不需要替代商业智能的能力。这两种能力是相辅相成的，每种能力都提供了一个必要的业务运营视图和运营环境。下图中： 商业智能与数据科学的比较， 标识了这两种能力之间的不同。关键的对比包括：

- __发现问题 vs. 预先准备的问题__：数据科学实际上是从事发现需要问的问题的工作而不仅仅是问问题。
- __多种能力 vs 单一能力__：整个团队提供了一个集合了计算机科学，数学和领域知识一起的公共讨论平台。
- __展望性 vs. 回顾性__：数据科学专注于从数据中获得可执行的信息而不是报告历史性的事实。

__向后看和向前看__

<table class="table table table-bordered table-striped table-condensed">
<tr>
<td>论断式推导</td>
<td>诱导式推理</td>
</tr>
<tr>
<td>论断式推导<br>向后看<br>将数据切片切丁<br>入库和装仓的数据<br>分析过去，猜测未来<br>创建报表<br>解析</td>
<td>诱导式和论断式推理<br>向前看<br>与数据进行交互<br>分布式，实时数据<br>预测和建议<br>创造数据产品<br>回答问题和创造新的问题<br>可执行的答案</td>
</tr>
</table>

#### 数据科学有什么影响？ 

当我们进入数据经济时， 数据科学对致力于取胜的组织提供有竞争力的优势，不论取胜是如何定义的。这一优势是通过改进决策来达到的。一位前同事喜欢将基于数据的决策描述为：如果你拥有完全的信息或者完全没有信息，那么你的任务就简单了 - 介于这两种极端之间的情况是麻烦的开始。他这里强调了一个严酷的现实：无论有没有信息，必须做出决策。

组织做出决策的方式已经演进了半个世纪了。在引入商业智能之前，决策只能根据根据直觉，大嗓门和最佳辩论。不幸的是，这种方式今天依然存在，甚至是某些组织进行行动的主要手段。我们建议永远永远不要为这些组织工作。 

对我们的经济来说幸运的是，大部分组织开始使用通过简单统计得到的真实信息来辅助决策。那些做得好的组织获得回报；做的不好的则失败了。然而，我们正致力于超越简单统计使得我们能跟上市场需求的节奏。可获得数据的快速膨胀以及支持访问和利用这相应数据规模的工具使得我们能够本质性的改变组织做决策的方式。数据科学对维持在日益增加富数据环境中的竞争力是十分必要的。就像应用简单统计学一样，拥抱数据科学的组织也将获得汇报而那些不这么做的组织想要跟上步伐将会面临挑战。更复杂一点来说，随着可获得的数据集变得越来越多，这些组织间的鸿沟将变得越来越大。图，数据科学的商业影响， 标出了值得组织拥抱数据科学的理由。

数据科学的必要性...

<table class="table table table-bordered table-striped table-condensed">
<tr>
<td>17-49%</td>
<td>组织提高10%数据可用性时生产率的提升率</td>
</tr>
<tr>
<td>11-42%</td>
<td>组织提高10%数据访问时资产收益率（ROA）的提升率</td>
</tr>
<tr>
<td>241%</td>
<td>组织使用大数据来提高竞争力时投资回报的提升率</td>
</tr>
<tr>
<td>1000%</td>
<td>部署贯穿几乎整个组织的分析能力，将日常运营与高层管理目标对其，并引入大数据时的投资回报的提升率</td>
</tr>
<tr>
<td>5-6%</td>
<td>当组织通过数据来驱动决策时， 组织的绩效改进率</td>
</tr>
</table>

...不完全，未来补充。


#### 现在有什么不同？

20年来，IT系统的都是按照同样的方式来建设的。我们把人分成运营业务的和管理基础设施的(因此他们将数据看作他们已经管理的另一件简单的事情). 

随着新技术和分析方法的降临， 这种人为的并且高度低效的对关键技能的划分变得不在是必须的。首先，组织可以直接将业务决策者连接上数据。这是简单的将数据从"需要管理的东西"转变成“有价值的东西”。 

在转型的过程中，组织面临严峻的抉择：你可以继续创建数据竖井来拼凑分割的信息或者统一你的数据来提炼答案。从数据科学的视角来看，数据竖井一个错误的选择，因为：(a) 没有最大化的利用所有可用的数据来帮助组织成功的机会成本，和(b)使用过时的数据处理路径的资源和时间成本。数据产品的收益包括：

- __机会成本：__ 因为数据科学是一个新兴的领域， 当竞争者在你之前贯彻数据科学并从中获得了价值，机会成本就会提高。如果不能学习客户需求并引导改变需求客户将不可避免的从你当前提供的产品和服务流失。 当竞争对手能够成功的利用数据科学来获取洞察其结果就是，他们能驱动差异化的客户价值主张并引领行业。
- __强化过程：__ 由于世界的互联性日益增强，巨量的数据被产生和存储起来。数据科学可以将数据转化成洞察来帮助改进现有流程。通过有效的结合从未有过的数据中的复杂关系可以极大的降低运营成本。这将导致更好的质量，更高的产量和更有效率的运营。

### 从这里开始：数据科学实际上是如何工作的？

>它不是火箭科学， 它是某种更好的 - 数据科学

我们不要拿自己开玩笑 - 数据科学是一个复杂的领域。他是困难，需智力投入的工作。这一工作要求巧妙的整合人才，工具和技术。但是作为一个战地指南，我们需要摒弃复杂性来提供一个清晰，且有效的方法来理解这个新世界。 

为此，我们将数据科学领域转换到一组简化的活动，如图 数据科学努力的四个关键动作  。数据科学纯化论者可能不会赞同这种方式，但是他们也不需要战地指南， 他们就坐在象牙塔里吧！在现实数据里， 我们需要清晰和简单的操作模型来帮助我们前进。


![](http://i3.piimg.com/6d5c69f72360a5cf.jpg)

<table class="table table table-bordered table-striped table-condensed">
<tr>
<td> 动作1：获取(Acquire) <br><br> 这一动作专注于你所需要的数据。给定数据的性质，这一动作的细节很大程度上依赖于你是谁和你要做什么。因此，除了强调这一活动的重要性和鼓励开阔的视野来看哪些数据能够和必须使用以外我们不会在这项动作上花费太多时间。 </td>
<td> 动作2：准备(Prepare) <br><br> 好的结果不是唾手可得的。其很大程度上决定于准备工作。 在数据科学中准备意味着处理数据以满足你的分析需求。这一阶段可能耗费大量的时间，但是这是值得投资的。其收益既有长期的也有短期的。</td>
<td> 动作3：分析(Analyze) <br><br> 这一动作占据了团队的最大的注意力。同时它也是最具挑战性和最令人兴奋的(在这个过程中你将看到很多"顿悟时刻")。作为四项动作中挑战最大和最棘手的动作，本战地指南专注于帮助你更好和更快的做好分析工作。</td>
<td> 动作4：执行(Act) <br><br>  每一个高效的数据科学团队都会带着一个目的来分析数据。这个目的就是把数据转化为行动。可执行且有影响力的洞察是数据科学的圣杯。然而将洞察转变成行动可能会被政治主导。这一动作很大程度上依赖于你组织的文化和特质， 因此我们将如何解决这些细节留给你自己。</td>
</tr>
</table>

__1.获取(Acquire)__

所有的分析都是从访问数据开始的，对于数据科学家来说这是不证自明的公里。但是这对于组织中存储，维护和拥有数据的人来说，存在某些显著的区别。

在我们谈数据获取之前，我们先来看看有什么正在变化。传统上，刚性数据竖井人工的定义了可获得的数据。换句话说，这些竖井成为了数据过滤器，只保留了很小一部分数据而忽略其他的。这一过滤过程给我们提供的是一个基于这些“幸存数据”的人造的世界视图，而不是一个展示了全部真实性和意义的视图。没有一个广泛的数据集，我们不可能把我们自己深入到数据的多样性中。相反，我们只是基于有限和受限制的信息来做决策。 

消除数据竖井的需求使得我们能够一起访问所有数据，包括多种外部数据源。拥抱现实其多样性是有益的而复杂性是可以接受的。这一思维观念创造了一种完全不同的方法，该方法在考虑组织内的数据时将其赋予了一个新的且独特的角色。对组织而言，数据代表了值得注意的新利润来源和使命感更高的机会。但是正如之前提到的，这个动作很大程度上依赖于形势和环境。我们只能给你普遍性的指南来帮助你确保获得最大的价值：

- __首先朝内看：__ 哪些数据是你能访问但没有使用的？ 这是很大一部分被过滤过程丢弃的数据，且可能具有很大的价值。
- __抛弃格式限制：__ 停止将你的数据获取仅限在结构化数据库领域。 相反，将非结构化和半结构化数据作为可用的数据源。
- __搞清楚有哪些缺失：__问问自己有哪些数据如果你能得到就能得到很不同的结果，然后去找到它！
- __拥抱多样性：__尝试整合可能与你的领域相关的可获得的公开数据源。

---

> 不是所有数据都是平等的
> 当年开始整合数据时，记住不是所有的数据都是平等的。组织有收集所有可以获得的数据的倾向。唾手可得的数据（轻易可访问或者容易获得的）可能收集起来很容易，但是无法保证这些数据是有价值的。专注于那些能为你的组织带来最高的ROI的数据。你的数据科学团队可以帮你鉴别出这样的数据。同样要记住你需要调和你需要的数据和你拥有的数据。如果数据不是你需要的，那么收集大量的数据是无用而代价高昂的。

---

__2.准备(Prepare)__

一旦你拥有了数据，你需要为分析来进行准备。

企业通常基于不精确的数据来做决策。数据烟囱（？？？）意味着组织可能存在盲点。他们无法看到全貌并且无法从整体上来审视他们的数据和挑战。最终将导致有价值的信息无法送达决策者。有研究表明有差不多33%的决策是在没有好的数据和信息的情况下做出的[10]。 

当数据科学家能够探索和分析所有数据，分析和数据驱动的决策将带来很多新的机会。从这些新机会中获得的洞察将显著的改变组织内的行动和决策过程。无论如何，取得了组织内所有的数据后必须进行准备。 

我们的经验一次又一次表明数据科学家为分析准备数据的最佳工具是湖 - 特别的说是数据湖[11]. 这是一个筹集，存储和整合数据的新方法，它能帮助组织最大化数据的价值。与把信息存储在离散的数据结构中不同，数据湖把、整合一个组织完成的数据到一个单一而宏大的视图中。它消除了昂贵而麻烦的的数据准备过程，即 抽取/转换/加载（ETL）过程，以及使用数据竖井的必要性。在数据湖中的全部信息对每个查询都是可用的， 而且都是立即的。 

__3.分析(Analyze)__

我们已经获取了数据，我们已经进行了准备，现在是分析数据的时候了。

分析行动需要的努力是所有数据科学活动中最大的。实际上，数据科学家的工作就是构建能从数据中创造价值的分析。在这一场景下分析是指使用专门的且可扩展的计算资源和工具从指数增长的数据中获得相关的洞察。这种分析通过评估情境、运营和行为数据来实时理解风险和机会。

因为数据湖的数据完整性和全面的可访问性，组织可以通过分析来找到某些连接和模式，它们指出了一些有希望的机会。 这一高速的连接分析是在数据湖中完成的，相反老式的抽样方法仅仅只是利用了很窄的一些数据切片。以前，要理解湖中有什么，你需要拿出数据并进行研究。而现在你可以潜入湖中，把你的分析方法放入湖中。图， 数据湖中的连接分析， 强调了深入湖中来发现新的连接和模式的概念。

![](http://i4.piimg.com/36f2ced738f5192c.png)

数据科学的工作涵盖分析目标的整个光谱 - 描述，发现，预测和建议等几个方面。分析能力的成熟度决定了能支持的分析目标。对一个组织而言，很多因素在决定每个目标的难度和适用性上扮演了关键性的角色。有一些因素是组织的规模，预算和决策者需要的数据产品的类型。分析成熟度的详细讨论可以在图  组织的数据成熟度 中找到。

除了需要付出最大的努力之外，分析动作也是最复杂的部分。数据科学的tradecraft是门艺术。 但是我们无法教你如何成为一个艺术家，我们只能分享一些可能帮助你成功的基础工具和技术。 整个 拆掉训练轮  这一章致力于分享我们在长期服务数不清的客户中学到的一些领悟。这包括数据科学产品生命周期的描述和分形分析模型（FAM）。 分析方法选择过程和与之相辅的分析方法选择指南这两章提供了对于数据科学中最大挑战的洞察 - 选择合适的技术来完成这一工作。 

__4.执行(Act)__

现在我们分析完了数据，是时候采取行动了。

利用分析结果是十分关键的能力。如何利用也是非常依赖于当前的情况。就像获取动作，我们能够期望的最好的办法是提供一些指导原则来帮助你构建最大影响力的输出。这里列了一些当我们需要展示我们结果时需要记在脑中的关键点： 

 - 对于那些相对来说缺少训练和准备的决策者，分析结果必须是合理的有意义的。
 - 分析结果必须使得最有意义的模式，趋势和例外是容易被看到和解释的。
 - 任何努力都必须使得数据的量化是精确的使得可以精准的解释和比较数据。
 - 获得分析结果的路径必须是清晰且引入入胜的，同时要能到追溯到数据。
 - 分析结果必须回答了真实的业务问题。

#### 组织机构中数据科学成熟度

上文讨论的四种动作提供了数据科学的一个简单视角。在每个新的数据科学探索中，这些动作都会重复进行。但是随着发展，这四种动作所需付出的努力多少会变化。例如，随着数据池中获取的数据和准备好的数据越来越多，那这些动作所需的工作量就少得多。这是成熟的数据科学能力的体现。

评估数据科学能力需要从一种细微不同的视角出发。我们使用“数据科学成熟度模型”作为一个通用框架来描述成熟度进程，以及组成数据科学能力的组件。这个框架适用于一个组织机构的数据科学能力，甚至对于一种特定解决方案，也就是一个数据产品，也同样适用。在成熟度的每个不同阶段，可以获取强有力的洞察。

![](http://i2.piimg.com/6edb05c33fc6a49c.png)

开始的时候还是数据孤岛（Data Silos）。这个阶段，还没有开展任何广泛的收集活动。对现有数据也没有认识或者不知道需要什么数据。建立数据科学能力的决策标志着向收集阶段（_Collect  _stage）过渡。最开始的努力集中在认识和收集数据方面。你会逐渐地拥有你需要的数据，进而可以在收集阶段花更少的努力。你现在开始描述（Describe）数据。注意：尽管在收集阶段花的时间显著减少，但是决不会完全停止。这是之前给出的四个动作的陈述------在新的分析问题出现，需要额外的数据，以及新的数据源可用的时候，你将继续聚集和准备数据。

从描述（Describe）发展到建议（Advise）阶段，数据科学的成熟度继续提高。在每个阶段，因为有了更强大的分析能力，组织机构可以处理更复杂的分析目标。正如收集（Collect）描述的，在每个阶段不会完全消失，只是花在他们上面的时间比例下降，新的更成熟的阶段也开始了。表The Stages of Data Science Maturity简单描述了成熟度的不同阶段。

| 阶段| 描述|例子|
| -----|:----:|:----:|
|收集|关注收集内在和外在的数据集|得到销售记录和相应的天气数据|
|描述|寻求提高或者精炼原始数据，也进行基础的分析功能，比如计数|顾客在地里位置或者邮政编码上的分布|
|探索|找隐藏关系，或者模式|消费类似的顾客里面有没有团体|
|预测|利用过去的观察预测未来|能不能预测某些顾客群体更可能购买那种产品|
|建议|定义可能的决定，优化，给出能获得最好产出的决定建议。|你的建议是为了对某些特殊群体做广告，以获得某些产品的利润最大化。|

成熟度模型为理解数据科学能力的成熟度提供了一种强有力的工具。组织机构不需要达到最高的成熟度获取成功。在每个阶段都能发现很大的收获。我们坚信，除非想获得产出，要不然是不会在数据科学中付出努力的-----也就是专注于建议（Advise）。这意味着在成熟度中每往前一步将驱使你向正确方向迈进。达到成功要求正确的过程、人、文化和实践模型-----一种强大的数据科学能力。在“创建数据科学能力需要什么？”这一节阐述这个主题。

很少有组织机构运作在成熟度的最高阶段------预测（Predict）和建议（Advise）阶段。探索（Discover）阶段仅仅预示组织结构能够集中在更高级的预测（Predict）和建议（Advise）活动。这是数据科学的新前沿。这是我们开始理解如何减小人类与计算机之间的认知差距的地方。达到了建议（Advise）阶段的组织机构将会具备真正的洞察力和真正的完全优势。


### 从这里开始：创建数据科学能力需要什么

数据科学的一切无非就是构建团队和文化

许多组织（无论商业组织还是政府组织）都看到了利用数据在解放运营效率、创建新的服务和体验以及驱动创新等方面的潜力。

不幸的是，太多的业务领导都是在花费很大价钱、产生一些混合结果的一次性的技术解决方案上进行投资，而不是投资在构建战略性的数据科学能力上。

数据科学能力会嵌入到企业的日常运营当中从而能够将组织的运转效率提高到下一个层级进而提高投资回报。

数据科学能力推动一个组织从进行一些普通的分析提升为在将分析洞察作为日常业务中的一部分。

当构建一个能力时，非常重要的一点是一个组织要首先确立它的分析目标（例如：通过分析需要达成什么），然后评估它们需要达成目标的准备程度-除了检查技术的准备程度外，还要检查组织的准备程度。

一个组织然后就可以对如何弥补这个缺口进行战略的选择，进而构建他们的数据科学能力。

#### 打造你的数据科学团队

构建数据科学能力的一个至关重要的部分是有正确的团队。在数据科学文氏图中，我们可以看到数据科学依赖于不同的技能。

计算机提供对数据驱动的假定进行测试的环境，因此计算机科学对于数据加工和处理是非常重要的。

数学对于检验数据科学问题提供理论体系。丰富的统计学、几何学、线性代数以及微积分对于理解很多算法和工具的基本原理都是非常重要的。

最后，领域专家帮助理解什么样的问题真正的需要解决，在领域中存在什么样的数据以及问题空间可能被如何的测量。

![](http://i3.piimg.com/71b137576265b604.png)

请记住数据科学是个团队运动。在大多数时候，你不可能发现稀有的“独角兽”- 那些具有所有这三项技能的专家。因此，打造一个混合的团队从而使得团队能够具有数据科学文氏图中的三项基本技能就变得非常重要。

---

> 平衡数据科学团队组成的方程式
>
> 平衡一个数据科学团队的组成就像对一个化学反应的方程式进行配平。
>
> 在方程式的两边的每个元素的量必须相同。
>
> 在数据科学这个方程式中，这些元素是基本的技术能力包括：计算机科学、数学以及领域知识。
>
> 这些反应物，也就是你的数据科学界，每个人都有自己的独特的技能。你需要平衡成员的组成从而满足一个数据科学团队所需要的技能，这就是化学反应的产出物。 如果你不能正确的进行方程式配平，你的数据科学团队就不会在组织中产生期望的影响。
>
>2 CS M2 + 2 CS + M DE → CS4 M5 DE
>
>在上面的例子中，你的项目需要4部分的计算机科学，5部分的数学以及1部分的领域知识。根据给定的成员的技能组成，需要5个人来完成这个方程式配平。在你的数据科学项目实施过程中，技能的需求可能会变化，你应该对方程式进行重新的配平从而保证反应的平衡。

---
#### 理解什么造就了一个数据科学家

数据科学经常需要在跨过不同的任务中投入非常多的时间。必须要先产生一个假设，然后要获取、准备、分析数据然后产生行动。在产生有意义的结果前，经常需要使用多个不同的技术。如果这些看起来令人畏惧，因为它的确令人畏惧。数据科学是困难的、繁重的智力工作，需要非常多的天赋：即包含实实在在的技能也包含一些不可感知的“x元素”。

有四个相互独立但是又综合的数据科学基本竞争力集，只有当把这四个竞争力集在一起考虑时，才能表示什么才能代表一个成功的数据科学家。当然还有其他的竞争力用来补充这四个基本的集合，不过它们并未在数据科学核心技术和属性中进行定义。

__数据科学竞争力框架__



<table class="table table table-bordered table-striped table-condensed">
<tr>
<td> 集合 </td>
<td> 竞争力</td>
<td> 描述</td>
</tr>
<tr>
<td> 技术：知道如何以及怎么去做 </td>
<td> 高等数学；计算机科学；数据挖掘和集成;数据库科学；研究设计；统计建模；机器学习；运筹学；编程和脚本</td>
<td> 技术竞争力集合描述对于一个工作或者角色如果要出色的完成所需要的基础的技术以及专业知识和技能</td>
</tr>
<tr>
<td> 数据科学咨询：“可以在客户环境中工作” </td>
<td> 合作和团队配合；沟通；数据科学咨询；伦理和诚信</td>
<td> 咨询竞争力集合中的特性可以帮助数据科学家比较容易的融入不同的市场或者领域环境并且与业务单元合作去理解环境并行解决复杂的问题</td>
</tr>
<tr>
<td> 认知：能够去做或者学习去做 </td>
<td> 批判性思维；归纳或者演绎；解决问题</td>
<td> 认知竞争力集合代表了一个数据科学家在完成他们的工作时所需要的批判性思维以及推理能力（归纳或者演绎）</td>
</tr>
<tr>
<td> 个性：“愿意并且主动的去做” </td>
<td> 适应力/灵活性；歧义容忍度；面向细节；创新和创造力；求知欲；毅力；弹性和坚韧；自信；职业道德</td>
<td> 个性竞争力集合描述了对数据科学家有利的个性特点，比如求知欲，创造力和毅力。</td>
</tr>

</table>

----
__全能独角兽__
 
>一个人如果在数据科学需要的三个基础技能方面都非常棒则就像独角兽一样 - 非常稀有，如果你有幸遇到一个则必须要小心的对待。

>当你管理这类人时：
>
> - 鼓励他们领导你的团队，但是不是管理它。不要把他们陷入到可以由其他人完成的管理职责当中
> - 对管理他们的职业和在你组织中的兴趣投入更多的精力。在你的组织内部给他们创造提升的机会，允 许他们致力于指导其他的数据科学家以及提高他们的水准从而推动他们的职业的发展。
> - 确保他们有机会在很多不同的论坛上展示展示和扩散他们的想法，但是要注意他们在这些活动上花费的时间 

----

数据科学家最重要的素质是他们的个性中的一些无形的特点。数据科学家是好奇的、有创造性的、专注的以及面向细节的。

- __好奇心__对于剖析一个问题以及检查它与表面上看起来不相关的数据的相互关系是必不可少的。
- __创造力__对于在解决一个问题时创造和使用一个在那个环境中从没有用到过的方法时是必须的。
- 当无数天或者无数周要设计和测试一个技术，发现它不奏效，从失败中学习然后进行下次尝试，没有__专注力__是不行的。
- __注意细节__对于保持严谨，在检查数据时发现和避免过于相信直觉是非常重要的。

我们发现一个唯一最重要的品质是在克服困难时的灵活性 - 愿意抛弃掉一个想法然后尝试新的方法。通常，数据科学是先走入大量的死胡同，最终找到正确的阳光大道。在这样的环境下，需要独特的个性品质才能成功。技术技能可以通过时间来培养，但是灵活性，耐心以及坚韧不拔则不能。

----
> 不要通过封面判断一本书，同样的不要通过一个人的学位判断一个数据科学家。神奇的数据科学家可以在任何地方被发现。可以看一下我们的专家的学位的令人吃惊的多样性：
> 
> 生物信息，生物医学工程，生物物理，商业，计算机图形学，计算机科学，英语，森林管理，历史，工业工程，信息技术，数学，国家安全研究，运筹学，物理，野生动物和鱼类管理

----

#### 发现你团队中的健将

打造一个数据科学团队是复杂的。组织必须同时利用组织内部的员工去创造一个能够用来招聘和扩大团队的“锚”，同时需要经历组织的变化以及转变从而能够更好的吸收新进入的员工。打造一个数据科学团队从在组织内部识别那些对数据科学有非常高的潜能的员工开始。好的候选人通常在我们提到的三个基本技能中的一个或者多个有正规的背景，并且非常重要的是有数据科学所需要的个性特征。他们经常具有比较高的学历（硕士或者更高），但是也不是全部都这样。你找到的最早期的员工还需要有领导力以及对组织的目标的感觉，因为他们需要领导后面加入的员工和进行招聘。不要对这些需要打任何的折扣-因为你将会在最奇特的背景组成的最奇怪的地方找到数据科学家。

#### 塑造文化
勿容置疑的是-打造一个文化是非常困难的，而且对于对科学和对艺术一样的困难。它是深思熟虑的创造一个使得数据科学（无论对于数据科学家还是普通员工）能够蓬勃发展的条件和环境。然后你就可以后退一步赋权给集体完成有机转化。

数据科学家通常都是充满好奇心和想象力的。我们经常对我们的团队说：“我们不是多管闲事的，我们是数据科学家”。这些素质是项目成功以及在面临挑战和问题时能够获取新思维的基础。通常数据科学项目都是因为缺乏对新的不同的东西的想象力所束缚。如果想要建立一个强大的数据科学团队，通常组织必须要培养所有层级之间都相互信任和透明沟通的的氛围，而不是因为权力而有所不同。管理者需要准备好更频繁的邀请团队成员参而不是频繁的给予解释。

对于数据科学俱乐部,非常重要的一点是提供一条道路从而使得普通的员工也对数据科学感到舒适和熟悉。对于组织文化中的部分，它必须也是员工日常行为组成中的一个部分。这意味着员工必须在他们的日常工作中使用并且与数据产品交互。另外一个塑造正确文化的要素是所有员工对于数据科学知识都需要一个基线，从一个通用的专业词典开始、从而帮助提高协作和提升自信。尽管不是所有的人都会是山上科学家，员工需要认同数据科学并且具备相应的知识、技能、以及与数据科学家一起工作的能力，从而能够驱动更加智慧的决策和产生指数级的组织效率。

需要时刻记住的是好奇心和想象力是数据科学的标志，他们是数据科学项目成功的基础。

#### 选择你的操作模型

根据规模、复杂度以及商业驱动，组织需要考虑三种数据科学操作模型中的一种：集中式，部署式以及分散式。这三种模样如下图的数据科学操作模型所示：

![](http://i4.piimg.com/0bb8b2543e6ce3f6.png)

__集中式数据科学__团队服务于组织的所有的业务。这个团队居于首席数据科学家之下并且所有的人都在一起。领域专家轮流到这个组织里来做一定量的工作去解决业务的一些问题。这个模型在数据科学资源有限的情况下可以提供比较高的效率，但是会带来与其他业务单元在数据科学人才方面的竞争的挑战。为了解决这个挑战，需要重视资源的管理以及对于组织如何识别和选择数据科学项目时要透明。

__部署式数据科学__团队会短期或者长期的派遣到业务单元中去工作。他们自主工作并且与领域专家在一个组一起工作去解决困难问题。在部署模型中，数据科学团队跨过多个业务单元协同发展支持，中间的领导者作为一个桥梁来解决跨组织的问题。然而，数据科学团队既需要承担业务单元的领导角色，也需要承担他们自己团队的领导角色，这有可能引起困惑和冲突。在这个模型中，需要着重去进行冲突管理以避免优先级的竞争。
 
__分散式数据科学__团队是完全将数据科学嵌入到每个业务组中并且变成该组织长期的一部分。对于一个已经将分析作为中心的领域或者业务单元，这种模型会比较适合。在分散式模型中，团队可以非常好的对业务单元高优先级需求进行相应。然而，由于缺乏中心的管理，它可能造成重复的软件或者工具。另外，资金最充裕的业务单元经常占用了全部的分析资源但是其他的业务单元则什么都没有-这样对整个组织则可能并不能产生最佳的影响。在这个模型中，建立一个跨功能的组是非常重要的，从而能够提升跨组织的管理和协调。

#### 如何产生里程碑

数据科学的工作可以在最基层通过几个人解决难题来开始，也可以在首席执行官、首席数据官或者首席分析官的指导下开始。无论用何种方式开始，都会遇到政治影响经常会是一个比解决任何技术的障碍更严重的挑战。为了与这个挑战进行斗争，通过里程碑来周明数据科学团队所能提供的价值是非常重要的。最好的达成的方法通常是通过数据科学原型或者概念验证。概念验证对于每个数据科学原型都是至关重要的，能够产生开启任何数据科学能力四个要素的里程碑：

__1.组织接受：__一个原型如果想要成功，每个参与到其中的个体必须愿意做任何他们能做的去使它成功。一个好的衡量收益的办法是与中层管理者会面，他们的视角经常代表了较大组织的视角。

__2.清晰的投入产出比（ROI)：__在选择一个原型问题前，确认这个分析的投入产出比(ROI)是清洗的并且能够被项目或者整个组织证明。结果通常需要先在ROI如何决定和衡量达成一致，从而使得收益是可以量化的。

__3.必须的数据：__ 在选择一个原型前，你必须首先要决定需要哪些数据，这些数据是否可以拿到以及需要花费多少时间和代价才能够拿到。需要重点主要的是组织并不需要所有的可能的数据-尽管有些缺失，他们仍旧能够产生成功的分析。

__4.限制复杂度和持续时间：__原型想要解决的问题需要在过分复杂和过分简单直接找到一个平衡。新的数据科学组织经常为了显示价值而选择高复杂度的项目。然而复杂度越高，失败的风险就越大。同时，如果问题解决起来非常简单，高层领导以及组织内的其他人可能就看不到数据科学的必要性。寻找那些可能从比较大的数据集，或者将过去从来没有关联起来的多个数据集入手的问题，而不是选择需要非常复杂分析方法的问题。在这些情况下，经常会有一些能够对组织产生重要价值的容易解决的问题。

### 拆掉训练轮 - 数据科学从业者指南：指导原则

以创新为中心的原则规范我们使用数据科学技能，正如创新和数据科学是高度联系在一起的。这些原则不难，可以快速地完全掌握，而且已经达成共识。从问题分解到执行，都应该贯彻这一原则。

----
> 专家建议:
> 
> 排除一种方案比验证其正确性更容易，因此应该集中精力去探索导致一种方法可能被排除的显著缺点。这让你可以把时间集中于探索真正可用的方案，避免钻进死胡同。

----

__乐于失败。__数据科学的核心是对想法进行实验。只有当你用新的想法和操作进行实验的时候，真正创新的解决方案才可以得到。在实验中，失败也是可以被接受的。在寻求解决方案的时候你不用去考虑哪些地方会失败。

__频繁失败和快速学习。__除了乐于失败，也应该做好重复失败的准备。为了找到一种可行的方案而探索十多种方法的事情是经常发生的。你不能陷入失败本身，而应该努力快速地从所做的尝试中进行学习。你能找到大量解决方案的唯一办法就是快速进行这项工作。

----
> 专家建议:
> 
> 如果你一开始就想得到最终解决方案，你会失败的，只有在多次碰壁后才会成功。

----
__牢记目标。__你可能经常陷入具体细节和执行的挑战当中，然后忘了你的目标，偏离了数据和分析目标的原始轨迹。定期地回过头看看，想想你的目标，并评估你当前的办法能否引导你到达目标。

__付出和专注是通向成功之路。__在找到有效的方法前，你得探索很多种方法。这很容易使人灰心，因此你必须为你定下的目标持续付出，专注于数据所揭示的细节和灵感。有时候表面看起来很小的观察结果可得到巨大的成功。

__复杂不等于更好。__作为技术人员，我们喜欢探讨高度复杂、进阶的方法。但是有时候更简单的方法可提供同样的解决方案，更简单意味着更容易更快速地进行原型化、实施和验证。

### 拆掉训练轮 - 数据科学从业者指南：推理的重要性

注意：在数据科学的世界中，长得像鸭子、叫起来像鸭子的也许只是一只麋鹿。

数据科学支持和鼓励在论断式（基于假设）和诱导式（基于模型）推理之间进行切换。诱导式推理和探索性数据分析有利于我们改进假设或发现新的分析途径。实际模型不需要是静态的，而是需要不断地测试、更新及改进，直到找到更好的模型。

----
> 没有因果关系的相关性 
> 
> 这种现象的一个常见例子是冰淇淋消费量和夏季谋杀率之间的强相关。这意味着冰淇淋消费导致谋杀，或者相反地，谋杀导致冰淇淋消费？很有可能不是，但是你可以看到由相关性误解因果关系的危险。作为数据科学家，我们必须确保懂得两者之间的区别。

----

大数据分析已经把归纳推理推到了时代前沿，我们需要进行大量的数据分析来找到相关性。但是，这种方法的一个常见缺陷就是得到令人困惑的因果关系相关性，这些相关性只暗示了因果关系，但是并没有得到证明。在理解关联数据之间的潜在机制之前，我们无法从相关性中得出结论。没有合适的模型来处理数据，相关性仅仅是一种巧合。

#### 拒绝的危险

在大数据时代，当面对那些实际上没有显而易见适用的模型的时候，我们经常拒绝分析。在统计学上，这种情况被称为I类错误。作为科学家，我们经常注意收集一些新的有趣的能够解释某种现象的突破进展。我们希望能够从我们的数据中找到一种可以解释一些事情，或者给出一个答案的模型。通过借助一个很小的值，假设检验可以完成限制I类错误这一首要目标。例如，值为0.05，则表明，在实际上不显著的时候，有1/20可能性检验仍然非常显著的。在检验多种假设的时候这种问题混合在了一起，I类错误就有可能显现。随着分析的时候数据越来越多，I类错误又必须得到控制。

我的一个项目要求检验两组微阵列基因数据样本之间的区别。微阵列数据包含成千上万的指标（受观测所限）。在不同条件下测量相同的基因是一种普遍的分析方法，如果两组数据在基因表达上有足够显著的不同，我们就认定这种基因与某种特定性状有关。一种惯常的做法就是获取基因性状的平均值，然后利用假设检验判断不同平均值之间是否存在显著差异。在执行成千上万组这样的检验后如果α=0.05，则可以找出某些显著的差异。当然这种方法的问题是------结果可能是由随机误差导致的。

有多种方法可用于修正显著性检验的错误推论。伯努利修正就是其中最保守的一种，这种计算可以降低拒绝零假设的标准(p值)的风险，公式就是alpha/n，这里n为你进行的假设检验的次数。因此，如果进行1000次显著性为α=0.05的检验，那么拒绝零假设的p值应该小于0.00005（0.05/1000），这显然是一个更加严格的值。大量先前显著值不再显著，揭示了数据内部的真实关系。

经过修正的显著性使我们相信，基因表达取决于细胞基因组本身的不同，而不是噪声。我们可以利用这种信息研究是什么蛋白质或方式影响基因表达。通过加深对因果关系的理解，我们把研究重点放在那些可以引导我们对基因功能有新发现的领域上，最终改进医疗技术。

推理和常识是数据科学的基础，没有这二者，数据也仅仅就是一堆数字而已。人们建立了带有偏差和假设的上下文环境、推理和模型，盲目相信分析结果是一件危险的事情，这有可能导致你得出一个错误的结论。当你在处理一个问题时，应该常停下来问问自己下面的几个问题：

- __正在解决什么问题？__把答案清楚的说出来，特别是在和用户沟通的时候。请确保听起来像一个答案，例如，“给定一定数量的人力资本，按照这种优先次序派遣将会得到最佳回报。”
- __这种方法有意义吗？__写出你的分析计划。按照写下的计划执行，因为这为你的思维提供框架。简单的计划也是你的方法存在的证据。没有这些准备，分析的时候会产生大量的糟糕答案。
- __答案有意义吗？__你能解释答案吗？计算机和小孩不一样，它们只做我们命令的事情，所以请确保按照你的意图操作电脑。把你的假设记录下来，这些假设不要带有任何偏见。
- __是发现还是错误？__对出乎意外的结果保持怀疑，经验表明，如果结果看起来是错误的，那有可能就是错的。在接受一个结论前，应该理解并可以清晰地解释为什么它错了。
- __分析在解决最初目的吗？__不要为了满足客户的期待而去套一个结论。实事求是，但是请记住，一个对苛刻问题的答案需要更多而不是更少的数据分析。
- __故事讲完了吗？__你所做的分析是为了讲一个经得起推敲的故事，不能靠观众把细节拼凑起来。把坑找出来填好，避免出现意外。语法、拼写和图标很重要；如果结果看起来马虎、潦草，观众将对你的分析失去信心。
- __未来的方向在哪里？__分析永远不会结束，你只是用尽了资源而已，如果有更多的资源，请理解和解释还有什么其他的措施可以采用。

建议：

	- 笔头胜过好记性，每天结束工作的时候记下来做到哪一步了，也许你能从这些笔记中获得一些东西。把你学到的和为什么改变计划记下来。
	- 找一个友好的观众测试一下你的结论，确保合情合理。
	- 找一个友好的旁观者测试一下你的结论，确保合情合理。

### 拆掉训练轮 - 数据科学从业者指南：数据科学的元器件

数据科学是由很多互相作用的元件组成，了解它们是如何组织起来的是提高解决数据科学问题的工程能力的关键。

数据科学中包含的成员归结为以下几类：分析的数据类型，使用的分析类型，调用的学习模型，还有进行分析的执行模型。如图《数据科学成员之间的相互关系》所示，成员之间的相互联系使得设计数据科学解决方案非常复杂。对一个成员所做的选择受到其他成员所做选择的影响。例如，数据类型决定了分析类型和学习模型的选取，时效性和算法并行化在后续会对执行模型产生影响。随着对数据科学技术领域研究的深入，我们开始探索这些成员，并探讨每个成员的例子。

----
__快速简略了解：__

当设计一个数据科学解决方案时，你应该首先从理解那些定义解空间的成员开始。不论你的分析目标是什么，你应该考虑如下的事情：使用何种数据类型，用来生成数据产品的分析类型，选用的学习模型是怎么计算和优化的，管理分析是如何执行模型的。只有在考虑了以上方方面面之后，你才会对一个完成的数据科学解决方案有清晰的概念。

----

![](http://i4.piimg.com/7daebf14db7ad077.png)

#### 数据类型(Data Types)

数据类型和分析目标就像小鸡和鸡蛋的关系一样，不清楚先有谁。分析目标来源于商业需求，但是数据类型也影响目标。例如，理解消费者产品认知的商业需求推动了情感分析的分析目标。同样的，情感分析的目标推动了一种如社交媒体内容的类文本数据类型的选择。在设计解决方案的时候，数据类型也推动了许多其他选择。

有很多方法将数据分类。通常将数据分为结构化数据和非结构化数据。当数据的信息具有清晰含义并且高度明确、有序和数字化时，它就是结构化数据。有些结构化数据无法用常规关系型数据库或数据表来存储，但是却带有标签等特征，这样的数据是半结构化的。非结构化数据（比如自然语言文本）没有这么多清晰描述的意义，图像、视频和音频也常归为其中，这种数据要求经过预处理识别和提取有用特征，以便进行索引、训练或聚类。

数据也可以按照产生、收集或者处理的速度进行分类。流数据和批数据之间的区别可以描述成：流数据像消防龙头喷出的水连续，批数据则像是桶装水批量到达。数据类型和数据速度之间鲜有关联，但数据速度对选择的分析执行模型有显著的影响，也可能左右分析类型或者学习模型的选择。

#### 分析方法的类别(Classes of Analytic Techniques)

为对经验领域中可能的分析方法有个概念了解，我们将它们分成九个基本类。注意：一个给定类中的方法可能以多种方式完成不同的分析目标。一个类中分析方法仅仅表示有一种相似的分析功能。这九种分析类如图《分析方法类》所示。 

![](http://i4.piimg.com/81a0d22a72268af8.png)

__转换分析__

- __汇总：__总结数据的方法。这些方法包括基本的统计学（如平均值、标准差），分布拟合，绘图。
- __丰富：__将额外信息加入数据中的方法，比如加入源信息或其他标签。
- 处理：__进行数据清洗、准备和分离的技术。同时也包括常见的预处理方法，如坐标转换和特征提取。

__学习分析__

- __回归：__估计变量之间关系的方法，包括了解哪些变量在预测未知变量时更加重要。
- __聚类：__把数据分到相似的组合中。
- __分类：__有监督地对未知分类的数据进行划分
- __推荐：__基于历史偏好或者行为，对一个新实体预测偏好或者倾向的技术。

__预测分析__

- __仿真__：模拟真实流程或者系统运作的技术。这对在新条件下预测行为很有用。
- __优化__：选取一组元素来使得目标函数最大化的方法。

#### 学习模型

诸如回归、聚类、分类和推荐等这些分析方法都会采用一个预测模型。这些模型着眼于如何训练历史数据来对新数据进行判断。学习模型也描述了它是如何判断和逐步优化的，如图表《分析学习模型》所示。 
![](http://i4.piimg.com/ef5ae858e856e4ec.png)

学习模型常常被分成非监督学习和监督学习。监督学习是模型使用一个带有标记的训练集，其中的每一个元素都有一个已知的类或者种类。模型把训练样例中的标记和特征关联，以便对那些没有标记的样例进行预测。非监督学习没有关于数据将会归为哪些类的先验知识。非监督学习基于特征相似性利用数据集中的特征生成几个组。半监督学习是监督学习和非监督学习的混合，即利用小部分带标记的数据和大量的未标记的数据。非监督学习是为了提高在训练时只有小部分带有标记的观测量可用于学习的时候的学习精度。

有多种不同的方法训练学习模型。一种实用的区分是离线模型和在线模型，离线模型一次训练完成；在线模型逐步训练。许多训练方法有离线和在线变种。选择哪一种取决于分析目标和选取的执行模型。

生成一个离线模型需要一次性应用全体训练集，而改进模型则需要逐步进行。训练出的模型是静态的，他们的预测结果也不会改变，直到后续训练阶段的新模型建立起来。因为这种确定性的特点，离线模型的性能更容易得到评估。运用这个模型到生产环境中只需替换模型即可。

在线模型一直在动态优化，这意味着只需将其单次部署到生产环境中即可。挑战是这些模型不能将所有数据用于训练。这些模型的假设是数据必须基于观察到的样例；这些假设可能不是最理想的。模型预测的反馈可以减轻次优预测的影响。在线模型可以迅速结合反馈改进性能。

离线学习的一种方式叫做强化学习。在这种方式下，算法适应环境，基于函数反馈渐进地学习如何达到目标。强化学习通常用于复杂的、涉及优化的现实世界中的任务，如导航或交易。得益于很多很有前景的强化学习算法结论的公开，这种方法在近几年与深度学习一道迅速得到流行。

----
>强化学习实战

> 《自然》杂志上发表过一份研究表明，计算机可以学会玩49种不同的视频游戏，精度足以与一个专业的游戏测试人员匹敌。强度学习的潜力获得了极大关注。计算机使用原始的屏幕像素和游戏分值作为输入来完成这些学习任务。这种方法代表了第一代能够学习高维感官输入到行为响应之间复杂任务的人工智能。

----
#### 执行模型

执行模型描述了如何处理数据来完成分析任务，他们可能处理高维度问题。执行模型包含在一个组织分析计算程序的执行框架中。从这层意义上看，一个框架可能与一门编程语言运行环境一样简单，如Python解释器，或一个为一门或多门编程语言提供专用API分布式计算框架，如Hadoop或Spark。基于如何处理数据来对执行模型分组很常见（分为批处理或者流处理模型），执行模型的种类如图《分析执行模型所述》。

![](http://i3.piimg.com/ca0ef02e7d40f8ad.png)

批处理模型意味着大批量地分析数据，它分为运行态和非运行态两种状态，这两种状态是在内存中维护的。批处理也意味着分析程序大约几分钟或者更多的频率生成结果。由于批处理相当于离散的几个工作单元，因此对其工作量相当容易掌握，所以，基于数据到来的速度，很容易确定一系列特定的执行步骤和合适的执行频率及时间范围。取决于所选择的算法，批处理模型易于通过并行化得到扩展。有很多支持并行批处理分析操作的框架。最著名的当属Hadoop在其MapReduce框架中提供的分布式批处理执行模型。

与之相反，流处理模型只要数据来了就分析。流处理模型在正常情况下是一直在执行分析任务的。分析一直存在于内存中，并且根据数据到来的时间，以差不多秒级的频率连续生成结果。流处理的很多概念来源于Unix管道设计理念；通过把一个步骤的输出作为下一个的输入，各个步骤都连接起来了。因此，很多开发者对流处理的基本概念都已经很熟悉了。有很多支持并行处理的流分析框架可以使用，如Storm，S4和Samza。

选择批处理还是流处理模型取决于分析延时和实时性要求。延时表示数据到达系统后所需要的分析时间，实时性表示系统给出分析结果平均时间。对很多分析目标，几个小时的延时和数天的实时性是可以接受的，因此，可以选择批处理方法完成。而某些分析目标有秒级的要求，几分钟的实时性就几乎没有价值，因此适合选择流处理方法。

批处理和流处理模型并不是对分析执行方法分类的唯一维度。另一种分类方法是按照可扩展性进行分类，很多时候可通过多台计算机并行计算来完成扩展。某些算法需要大量内存来共享状态，而其他的算法在计算机之间没有共享状态，在构建一个并行分析执行环境的时候，这种区别对软硬件的选择有显著影响。

Tips:为理解流处理的系统能力，我们总结了如下指标：数据消费量，数据生成量以及延时。这将帮助你应对资源紧张的局面。

### 拆掉训练轮 - 数据科学从业者指南：分型分析模型

数据科学分析非常像椰菜

分型是表现出自相似模式的数据集，当你对一个分型进行放大时，相同的模式再次出现。设想椰菜的茎,撕下一片椰菜，这片和原来的茎很像。依次类推，更小片的椰菜仍然看起来像原来的茎。

数据科学分析很像椰菜，从时间和结构上看其本质都是分形。分析的每一个版本都遵循相同的开发过程，对任意一个给定的版本，分析本身是一个更小的可以继续分解的分析集。

#### 自然迭代

好的数据科学是时间维度上是一个分形的迭代过程。迅速获得一个不完美的解决方案比一个无法实现的完美解决方案可以得到更多收获。图《数据科学产品周期》总结了数据科学产品的生命周期。
![](http://i3.piimg.com/35af79f63b9c51f1.png)

设置是基础，汇总和准备数据，与领域内的专业知识结合。在子集合上尝试不同的分析方法和模型。评估模型、改进，再次评估，然后选择一个模型。用你得到的模型和结果做一些工作------把模型部署到实际操作中。评估商业效果，以便改进整体产品。

#### 更小的椰菜：一个数据科学产品

数据科学产品的各方面组成将随着每次迭代变化。让我们看看一个数据科学产品的幕后，检查每次迭代过程中的组成。

为完成一个更大的分析目标，需要将问题分解成子模块，再逐一解决。图《分型分析模型》说明了将数据科学产品分解成四个模块。

![](http://i3.piimg.com/d8ac9f2fdaf37c2d.png)

__目标__

你必须对你的分析目标和分析的最终状态有一些想法。是发现，描述，预测，或者建议？也可能是以上这些目的的组合。在开始分析之前，你应该明确数据的商业价值以及如何利用数据来驱动决策，或者以无实际意义但是很有趣的结果收尾。

__数据__

数据决定了分析可以提供的潜在模式。数据科学是关于在可变的数据中找到模式并比较这些模式的科学。如果数据对你要分析的事件没有代表性，你就想通过A/B Test或者实验来收集有代表性的数据。数据集绝对不会是完美的，所以不要等待完美的数据来了才开始工作。一个好的数据科学家善于处理那些包含丢失或者错误值的脏数据。确保在清洗数据上花点预留时间，否则就冒着产生一个垃圾结果的风险。

__计算__

通过建立洞察的过程，计算与数据一道完成目标。通过分解和攻克，计算将分析分解成几个更小的具有各自目标、数据、计算和结果操作的分析功能，就像一片小的椰菜保持了一株整体椰菜的完整结构。在这个意义上，计算本身也是分型。能力构建模块利用不同类型的执行模块，如批计算或者流计算各自完成小任务。小任务经合理的关联后可以得到复杂的有实际意义的结果。

__执行__

工程师应该怎样改变操作过程以得到更高的产出？保险公司该怎样选择为不同的人提供不同的方案和价格，计算的结果应该使执行与数据产品的目标保持一致。那些不支持或不指导行动的结果是有趣但是没有任何意义的。

在时间与结构上都是分形的数据科学分析过程中，有很多机会供我们去选择精巧或粗糙的分析构建模块。分析选择过程将会对此提供一些建议。

### 拆掉训练轮 - 数据科学从业者指南：分析方法选择流程

如果你只专注于数据科学的科学层面，你绝不会成为一位数据艺术家。

数据科学中极重要的一步就是发现一种可得到期望结果的分析方法。有时这显而易见；问题的特征（例如，数据类型）指明了你应该采用的方法。但有时无从下手。由于可能解决问题的分析方法实在太多了，从众多方法中选出一种是一门必须经过训练的艺术。接下来，我们将和您一起进行实践，并为您成为一名数据艺术家保驾护航。

#### 分解问题 

分析方法选择流程的第一步就是把问题分解成易处理的几个部分。为了达到预期效果，要求把多种分析方法整合成一种整体的方案。策划完整的方案要求把问题分解成前后连续的更小的子问题。

__分形分析模型__就是基于上述理念的一种方法。该方法的核心思想是，在任何给定的阶段，分析方法本身是由更小的计算集合组成，这个集合中的元素还可以继续分解。当问题分解的到一定粒度，仅需一种单一的分析方法就可以达到分析目标。问题分解产生了很多子问题，每个子问题都有自己的目标、数据、计算和执行。下图展示问题分解这一概念。

从表面上看，问题分解是一个机械、重复的过程。当然，从概念上讲，这个看法是正确的，不过在解决工程问题的时候，问题分解却是一把利刃。也许有很多不同的方式去分解问题，每个子问题有不同的解决方案。也许很多隐藏的依赖和制约只有在你已经开始寻找解决方案的情况下才会出现，这就是艺术与科学的碰撞。尽管隐藏在问题分解背后的艺术是不可言传的，我们仍然可以提炼一些有用的建议给大家。当你开始分解问题的时候，你可以考虑从如下几个角度入手：

1.建立自然组成的混合分析目标。例如，很多问题对包括探索和预测目标在内的预测未来环境很关注。

2.分析目标的自然排序：例如，在特征抽取的过程中，你需要首先确定候选特征组，然后选取蕴含最大信息量的那些特征。这两个步骤分别来自不同的目标。

3.关注处理过程的数据类型。例如，文本和图像都需要特征抽取。

4.需要人工参与反馈。例如研究预警阈值的时候，你可能需要征求分析员的反馈，基于他们的判断更新阈值。

5.合并数据源。例如，有时我们需要关联两个数据集才能完成更大的目标，往往是一个探索性目标。

问题分解除了提供一种易于实现的分析选择方法，它还具有简化高度复杂问题的好处。分解后的计算过程可以去解决每个子问题，而不必着眼于整体。不过需要注意的是，虽然这一技术有助于数据科学家解决问题，但当它作为完整的端对端解决方案时，必须进行评估。

>__识别域名欺诈__
>
>识别域名欺诈对一个公司来说是特别重要的，这有利于他们维护自己的品牌形象，并免遭客户信任度大打折扣。当用户打开由黑客创建的一个恶意网站、url或者邮箱地址并信以为真时，域名欺诈就发生了。当用户点击这个链接，访问相应网站，或者接收邮件时，他们将遭受某种恶意行为。
>
>![](http://i4.piimg.com/1f082a3de1418345.png)
>
>我们团队曾经为一个商业公司处理过这种问题。从表面上看，解决域名欺诈看起来很简单。只需找到最近注册的域名，检查这些域名与该公司的域名是否有较大相似性。当相似度达到一定阈值，我们便可以对该域名进行警告。但是在分解问题时，主要计算过程很快变得很复杂，我们需要一种判断两个域名相似性的算法。当分解这个相似性算法时，我们主要关注算法复杂度和速度。正如许多与安全相关的问题一样，快速警报的速度是至关重要的，对响应速度的强需求迫使我们重新评估分解识别域欺诈的做法是否适当。

重温分解过程后我们得到一种全新的方法。我们首先算出一个与该公司注册域名相似的域名列表，然后将其与最近注册的域名进行比较。上图展现了问题分解在处理域名欺诈过程中的应用。经测试和初步部署，我们可以在48小时内发现欺诈域名。

#### 实现约束 

在域名欺诈的案例研究中，实现约束迫使我们重新审视最初的做法。这表明分析方法选择并不仅仅意味着选择某种分析技术来实现所期望的结果，同时确保解决方案是可行的。

数据科学家可能遇到各种各样的实现约束。可以将其归纳为5个维度，分别为：分析复杂度，速度，精度，数据大小和数据复杂性。平衡这些维度是一个零和问题，一个解决方案不能同时在所有的五个维度都表现优秀，而是必须在它们之间进行取舍平衡。下图的平衡五维度分析，说明了这种关系。 

>![](http://i2.piimg.com/ddbd5cc03a97aef1.png)
>
>速度：结果生成时延（准实时、小时级、天级）或解决方案的开发、部署时间
>
>分析复杂度：算法复杂度(e.g., complexity class and execution resources)
>
>数据大小：数据集大小（比如数据行数）
>
>数据复杂度：数据类型，数据复杂度需要衡量数据重叠、线性可分、数据维度以及数据集之间的联系。

当问题的某个方面对这些维度的一个或者多个有要求时，就会面临实现约束的问题。数据科学家通常被迫牺牲其他维度来解决某个特定维度。例如，如果需要准实时地给出分析结果，那么对速度维度提出了要求，其他四个维度将被会被迫做出牺牲。了解如何平衡五个维度是一门艺术，同时需要一段时间的训练来完成。

一些实现约束的例子包括：

- 计算频率：一些解决方案可能需要定期执行（例如，每小时），这就要求在指定的时间窗内完成计算。如果不能在要求的时间内完成，那么就算是分析方法特别合适，也是无用的。

- 时效性：有些应用需要准实时地给出分析结果, 要求使用流处理方法。虽然有些算法可以在流计算框架中完成，但是很多其他的算法却不能。

- 实施速度：为了能够快速给出大致的分析结果，我们需要快速开发及部署解决方案。这种情况下，我们可以用一些不太复杂的技术来快速实施及验证结果。

- 计算资源限制：虽然你可能能够存储及分析数据，但是当数据量较大时，一些需要在整个数据集上进行多次计算的算法将会耗费很多资源。这将引领我们去使用一些单次使用数据的算法（例如，选用canopy聚类而不选用kMeans）

- 数据存储限制：有些时候，数据量会变得大到无法保存，或者只能保存一个较短时间窗内的数据。对于那些需要分析很长时间窗内的数据的算法来说，这是不可行的。

政策和监管要求是隐含限制的主要来源，值得简单讨论。针对诸如个人身份信息（PII）或个人健康信息（PHI）这种专门的数据，都有相应的政策限制。尽管现在的技术在一个单一系统中提供各种安全控制，可以安全的存储信息，但是政策强制要求有专门的数据处理考虑，包括有限制的保存周期和数据访问方式。数据限制条件对事先规划的数据大小和复杂度维度有影响。这就造成了必须考虑的另一层限制。

### 拆掉训练轮 - 数据科学从业者指南：分析方法选择指南

由于你无法凭感觉准确认知整个数据科学领域，所以我们为你绘制了一个导图。

分析技术这个领域是无比广阔的，也是很难理解的。我们用下面这个图来帮助你找到分析数据的方法。首先，我们回答如何认知分析目标及确认问题特征等相关问题。这些答案将引导你了解数据分析技术，并对相关技术做一些思考。

![](http://i2.piimg.com/13f1b78d257d7bdc.png)

1.有很多原因需要我们进行数据降维：

- 模型不收敛

- 模型输出的结果相当于随机结果

- 在当前多维度特征空间中，无法获取足够的计算资源以完成特定目标

- 不知道数据的哪方面是最重要的

2.__特征提取__是一个广泛的话题，并且与具体问题高度相关，这一主题展开说可以独自成书。所以我们将其从该图中删除。请参阅战壕的生活中的特征工程和特征选择章节。

3.应当检查数据标签的正确性，尤其是时间戳，因为它很可能恢复成系统默认值。

4.巧妙的丰富可以极大提高计算时间，也可能成为在不同的端到端分析解决方案之间的巨大差异。


- DESCRIBE 描述：通过何种方式去认识数据？
	- PROCESSING 处理:如何进行数据清洗？
		- FILTERING: 过滤  
		如何根据绝对或者相对值识别数据？
			- 如果你想基于自身的值添加或者删除数据，
				- Relational algebra projection and selection 可以利用关系代数投影和选择
			- 如果原始数据是无信息或者重复的，可以考虑：
				- Outlier removal 去除离群点
				- Gaussian filter 高斯过滤
				- Exponential smoothing 指数平滑
				- Median filter 均值过滤
		- IMPUTATION: 缺失值填充  
		如何填充缺失值？
			- 如果从数据集中该特征的其他值来确定缺失值
				- Random sampling 随机采样
				- Markov Chain Monte Carlo (MC) MCMC 采样法
			- 如果不通过其他观测值来确定缺失值，可通过
				- Mean 均值
				- Regression models 回归
				- Statistical distributions 统计概率分布
		- DIMENSIONALITY RDDUCTION:降维  
		如何降低维度？
			- 如果需要判断是否存在多维相关
				- PCA and other factor analysis PCA 主成分分析或其他因素分析方法
			- 如果能够以一个组内的关系单独观察，从如下开始：：
				- K-Means 聚类
				- Canopy 聚类
			- 如果是非结构化的文本数据
				- Term Frequency/Inverse Document Frequency (TF IDF)
			- 如果现有算法只能使用特定数量的特征，而现有特征比较多：
				- Feature hashing 特征散列
			- 如果不确定哪个特征比较重要：
				- Wrapper methods Wrapper 方法
				- Sensitivity analysis 灵敏度分析
			- 如果需要增加对于当前数据空间概率分布的理解
				- Self organizing maps
		- NORMALIZATION & TRANSFORMATION 标准化
		如何对数据去除重复数据？
			- 如果怀疑数据元素重复
				- Deduplication 重复数据删除 
			- 如果希望数据取值在某个特定区间
				- Normalization 数据标准化 
			- 如果数据以二进制存储
				- Format Conversion 格式化转换
			- 如果在频域进行计算
				- Fast Fourier Transform (FFT) 快速傅立叶变换(FFT)
				- Discrete wavelet transform 离散小波变换
			- 如果在欧几里德空间进行计算
				- Coordinate transform 坐标变换
		- FEATURE EXTRACTION:特征抽取
	- 聚合 如何收集和总结数据?
		- 如果对数据集不熟悉，从基本的统计学开始: 
			- Count 计数
			- Mean 平均
			- Standard deviation 标准差
			- Range 范围
			- Box plots 直方图
			- Scatter plots 散点图
		- 如果方法假设数据服从某个分布: 
			- Distribution fitting 分布拟合
		- 如果想理解数据所有可用信息:
			- Baseball card” aggregation “棒球卡”式聚合
	- 丰富 如何往数据中添加信息?
		- 如果需要跟踪源信息或者其他用户定义的参数:
			- Annotation 标注
		- 如果某些数域需要一起处理，或者使用一个数域计算其他域的值:
			- Relational algebra rename 相关代数
			- Feature addition 特征增加(例如地理、气候、天气)

- DISCOVER 探索 数据中的关键关系是什么？
	- CLUSTERING 聚类:如何发现数据自然分组？
		- 如果希望按某种层次来对数据进行聚类：
			- Hierachical 层次聚类法
		- 如果未知聚簇个数：
			- X-Means 算法
			- Canopy 算法
			- Apriori 算法
		- 如果是文本数据：
			- Topic modeling 主题模型
		- 如果想获取任意形状的聚簇：
			- Fractal 聚类
			- DB Scan 聚类
		- 如果希望获取聚簇内数据的软关系：
			- Gaussian mixture models 高斯混合模型
		- 如果已知聚簇个数：
			- k-Means(通过 Canopy 或者层次聚类来获取聚类个数)
	- Regression 回归: 如何确定哪个变量比较重要？
		- 如果数据具有未知结构：
			- Tree-based methods 基于树结构的一系列方法
		- 如果比较重视统计衡量方法的重要性：
			- Generalized linear models 广义线性模型
		- 如果不侧重统计衡量方法的重要性：
			- Regression with shrinkage (e.g., LASSO, Elastic net) 收缩回归
			- Stepwise regression Stepwise 回归
	- HYPOTHESIS TESTING 假设检验: 如何验证算法？
		- 如果想比较两组结果的差别：
			- T-test T 校验
		- 如果想比较多组的结果的差别：
			- ANOVA
- PREDICT 预测: 可能的输出结果会是什么？
	- CLASSIFICATION 分类 如何预测组关系？
		- 如果已知变量间的依赖关系: 
			- Bayesian network 贝叶斯网络
		- 如果不知道特征重要性: 
			- Neural nets 人工网络
			- Random forests 随机森林
			- Deep learning
		- 如果需要高度透明的模型: 
			- Decision trees 决策树
		- 如果维度小于 20: 
			- K-nearest neighbours K 近邻算法
		- 如果待分类的数据量较大: 
			- Native Bayes 朴素贝叶斯
		- 如果想通过已观测值来预测未观测值: 
			-  Hidden markov model 隐马尔可夫模型
		- 如果上述步骤后还是不知道如何开始: 
			- Support vector Machine(SVM) 支持向量机
			- Random forests 随机森林
	- REGRESSION 回归 如何预测一个未知量？
		- 如果数据结构未知: 
			- Tree-based methods 基于树结构的一系列方法
		- 如果比较重视统计衡量方法的重要性: 
			- Generalized linear models 广义线性模型
		- 如果维度小于 20: 
			- K-nearest neighbours K 近邻算法
	- RECOMMENDATION 推荐 
		- 如果你仅仅知道人与物品之间是如何互动的: 
			- Collaborative filtering 协同过滤
		- 如果知道物品本身的特征向量: 
			- Content-based methods 基于内容的方法
		- 如果你只知道物品间相互联系: 
			- Graph-based methods 基于图形的方法
- ADVISE 建议: 应该采取什么样的行动？
	- LOGICAL REASONING 逻辑推理 如何通过不同的证据进行推断
		- 如果你有专业的知识: 
			- Expert systems 专业系统
		- 如果你只需要基本事实: 
			- Logical reasoning 逻辑推理
	- OPTIMIZATION 优化 	
	 - 当目标可表达成一个效用函数，如何找到最好的行动方针？
		- 如果问题是由非确定性效用函数来表示: 
			- Stochastic search 随机搜索
		- 如果近似解是可以接受的: 
			- Genetic algorithms 遗传算法
			- Simulated annealing 模拟退火
			- Gradient search 梯度搜索
		- 如果问题是由确定性效用函数来表示: 
			- Linear programming 线性规划
			- Integer programming 整数规划
			- Non-linear programming 非线性规划
		- 如果你想尝试多种模式: 
			- Ensemble learning 集成学习
	- SIMULATION 模拟 如何通过不同的证据进行推断
		- 如果需要建立离散实体模型: 
			- Discrete event simulation (DES) 离散事件仿真
		- 如果有一组离散可能的状态: 
			- Markov models 马尔可夫模型
		- 如果有独立的实体之间的动作和互动: 
			- Agent-based simulation 代理人基模型
		- 如果不需要离散实体模型: 
			- Monte Carlo simulation 蒙特卡罗模拟
		- 如果正在模拟一个反馈机制与执行之间的复杂系统: 
			- Systems dynamics 系统动力学
		- 如果需要持续跟踪系统行为: 
			- Activity-based simulation 基于活动的模拟
		- 如果你了解什么样的因素驱动系统: 
			- ODES
			- PDES
		- 如果类别不准确: 
			- Fuzzy logic 模糊逻辑

<br/>

### 拆掉训练轮 - 数据科学从业者指南：分析方法明细表

仅带你到一个正确的出发点是不够的。我们还要让你理解你所看到的。

明白几种可能适用于你的问题的分析技术是非常有用的，但是只提供他们叫什么就起不了太大的作用。分析方法明细表介绍一些这些方法的具体知识。一旦你明确了将一种方法用在分析选择中，就找到表中对应的行。你会找到方法简要的介绍，我们所知的一些建议和一些有用的参考文献。

| Technique（技术）| Description（描述）|Tips from the Pros（Tips）|References we love to read（参考文献）|
| -----|:----:|-----|:----:|
| 主动学习| 通过智能样本选择来提高模型的性能。应当选择能够为学习模型提供最大信息的那些样本|可以和人工参与的循环结合来帮助获取领域知识|Burr, Settles B. “Active Learning: Synthesis Lectures on Artificial Intelligence and Machine Learning.” Morgan & Claypool, 2012. Print.|
| 基于代理人的模拟|模拟具有自主意识个体的行为和交互 | 在很多系统中，复杂行为是由令人惊讶的简单规则导致的。逐步建立代理，并让其保持简单的逻辑 | Macal, Charles, and Michael North. “Agent-based Modeling and Simulation.” Winter Simulation Conference. Austin, TX. 2009. Conference Presentation.|
|方差分析|用于对两组以上数据之间的差异进行假设检验|在使用模型前先检验假设，进行多重检验时留意家族模样差别（FWE）|Bhattacharyya, Gouri K., and Richard A. Johnson. Statistical Concepts and Models. Wiley, 1977. Print.|
|Apriori关联挖掘|用于发现频繁项集的一种数据挖掘技术|用于帮助了解元素之间的潜在关系|Agrawal, Rakesh, and Ramakrishnan Srikant. “Fast Algorithms for Mining Association Rules.” Proc. Of 20th Intl. Conf. on VLDB. 1994. Conference Presentation.|
|贝叶斯网络|对条件概率进行建模，可以看成一个有向无环图|使用更大模型前先手算理解原理|Russel, Stuart, and Peter Norvig. “Artificial Intelligence: A Modern Approach.” Prentice Hall, 2009 Print.|
|协同过滤|也被称为“推荐”，通过比较用户对物品的表现来建议或者取消物品。基于相似的物品或者相似的人所使用的物品来进行推荐。|当所解问题存在潜在的因素时（比如在电影领域），使用奇异值分解。|Owen, Sean, Robin Anil, Ted Dunning, and Ellen Friedman. Mahout in Action. New Jersey: Manning, 2012. Print.|
|坐标转换|通过不同视角观察数据|改变数据坐标系统，例如使用极坐标或柱面坐标，可以更重点地突出数据结构。坐标变换过程中的关键一步是保持数据的多维性，及系统分析数据子空间。|Abbott, Edwin A., Flatland: A Romance of Many Dimensions. United Kingdom: Seely & Co., 1884. Print.|
|深度学习|通过学习低层特征形成更加高级的学习的方法。经常表现为层级较多的神经网络。|利用GPU来有效地训练这一复杂模型|Bengio, Yoshua, and Yann LeCun. “Scaling Learning Algorithms towards AI.” Large- Scale Kernel Machines. New York: MIT Press, 2007. Print.|
|实验设计|采取可控实验以量化输入对输出的影响|部分因子设计可以显著减少不同类型的实验数量|Montgomery, Douglas. Design and Analysis of Experiments. New Jersey: John Wiley & Sons, 2012. Print.|
|微分方程|用于描述函数与导数之间的关系。例如随时间改变的关系。|微分方程可用于构造模型并做出预测。方程本身可以进行数值求解，并以不同的初始条件进行测试达到研究系统轨迹的目的。|Zill, Dennis, Warren Wright,and Michael Cullen. Differential Equations with Boundary-Value Problems. Connecticut: Cengage Learning, 2012. Print.|
|离散事件仿真|模拟事件的离散序列，其中每个事件只在特定的时间点发生。模型只在时间发生的时间点更新。|在分析基于事件的过程时，如生产线和服务中心需要确定不同的工艺参数变化对系统的影响，离散事件仿真是有用的。将优化和仿真结合可以提高提高效率。|Burrus, C. Sidney, Ramesh A. Gopinath, Haitao Guo, Jan E. Odegard and Ivan W. Selesnick. Introduction to Wavelets and Wavelet Transforms: A Primer. New Jersey: Prentice Hall, 1998. Print.|
|离散小波变换|将时间序列数据转换到频域并保存原始信息|提供了一种很好的时频局部化特性。傅立叶变换的优点是保留了频域和局部性|Burrus, C.Sidney, Ramesh A. Gopinath, Haitao Guo, Jan E. Odegard, and Ivan W. Selesnick. Introduction to Wavelets and Wavelet Transforms: A Primer. New Jersey: Prentice Hall, 1998. Print.|
|学习集成|学习多个模型，并将其输出结果组合起来以获取更好的性能|不要设置过多的模型参数和过度训练，否则会造成过拟合。|Dietterich, Thomas G. “Ensemble Methods in Machine Learning.” Lecture Notes in Computer Science. Springer, 2000. Print.|
|专家系统|使用符号逻辑去推理事实。模拟人类推理过程。|以人类易懂的解释为什么系统得出结论|Shortliffe, Edward H., and Bruce G. Buchanan. “A Model of Inexact Reasoning in Medicine.” Mathematical Biosciences. Elsevier B.V., 1975. Print.|
|指数平滑|用于去除由采集误差或异常值带来的失真|相比于使用移动平均方法对过去观测值的几何平均，指数平滑方法处理时对每个值分配一个随时间衰减的权重。|Chatfield, Chris, Anne B. Koehler, J. Keith Ord, and Ralph D. Snyder. “A New Look at Models for Exponential Smoothing.” Journal of the Royal Statistical Society: Series D (The Statistician). Royal Statistical Society, 2001. Print.|
|要素分析|以降低不可观测变量（也就是要素）数量为目的，描述相关变量的可变性。|如果怀疑某些因子对数据有不可估量的影响，那么可以尝试使用因子分析|Child, Dennis. The Essentials of Factor Analysis. United Kingdom: Cassell Educational, 1990. Print.|
|快速傅立叶变换|有效地将时间序列从时域转换到频域，也可以通过空间变换来进行图像增强。|在频域中可以更有效地过滤时变信号。此外，噪声常可以通过观察在异常频率点的信号强度确定。|Mitra, Partha P., and Hemant Bokil. Observed Brain Dynamics. United Kingdom: Oxford University Press, 2008. Print.|
|格式转换|创建一个标准的数据表现方式而不用考虑原始数据格式。例如，从微软Word或者PDF中（2进制文件）抽取raw UTF-8数据。|有很多开源软件支持数据格式转换，并且支持很多不同的格式。一个好用的软件包就是Apache Tikia。|Ingersoll, Grant S., Thomas S. Morton, and Andrew L. Farris. Taming Text: How to Find, Organize, and Manipulate It. New Jersey: Manning, 2013. Print.|
|模糊逻辑|允许描述真实性等级的逻辑推理|当类别没有清晰定义时使用。例如“暖”、“冷”、“热”等概念在不同的温度和情况下有不同的含义。|Zadeh L.A., "Fuzzy Sets.” Information and Control. California: University of California, Berkeley, 1965. Print.|
|高斯过滤|删除噪声或者模糊数据|可用于从图像中去除色斑数据|Parker, James R. Algorithms for Image Processing and Computer Vision. New Jersey: John Wiley & Sons, 2010. Print.|
|广义线性模型|当误差不是正态分布时，允许把普通线性模型扩展为广义模型|当系统中的观测误差不服从正态分布时使用|MacCullagh, P., and John A. Nelder. Generalized Linear Models. Florida: CRC Press, 1989. Print.|
|遗传算法|通过改变操作和参数变化引起逐代进化改进候选模型|在考虑参数组合时，增加代数会加大多样性，但是需要更多的目标函数评估。在一代中计算个体数可以同时进行。候选解决方案的描述能影响表现。|De Jong, Kenneth A. Evolutionary Computation - A Unified Approach. Massachusetts: MIT Press, 2002. Print.|
|网格搜索|对参数探索问题的离散值进行系统搜索|网格参数用于参数可视化及评估是否存在多个极小值|Kolda, Tamara G., Robert M. Lewis, and Virginia Torczon. “Optimization by Direct Search: New Perspectives on Some Classical and Modern Methods.” SIAM Review. Society for Industrial and Applied Mathematics, 2003. Print|
|隐马尔可夫模型|通过确定离散隐含变量对序列数据建模，但可观测量可能是连续或者离散的。|隐马尔科夫模型一个最强大的特点是展现一定程度的局部变换（压缩或者延伸）时不变性。然而，它的一个明显缺点是它描述系统在一个给定状态下保持不变的时间分布。|Bishop, Christopher M. Pattern Recognition and Machine Learning. New York: Springer, 2006. Print.|
|层次聚类|基于连通性的聚类方法，在数据集中依次建立更大（凝聚）或更小（分裂）的聚簇。|按数据的亲密程度形成聚簇。在解决数据量较大的问题时，算法的复杂度为O(N2)或O(N3)|Rui Xu, and Don Wunsch. Clustering. New Jersey: Wiley- IEEE Press, 2008. Print.|
|KMeans XMeans|基于质心的聚类算法，K表示聚簇个数，X表示事先不知道聚簇个数。|应用这个聚类技术时，一定要先了解数据的形状。如果数据不是圆形或者椭圆形，这一技术的返回结果将比较差。|Rui Xu, and Don Wunsch. Clustering. New Jersey: Wiley- IEEE Press, 2008. Print.|
|线性规划，非线性规划，整数规划|在一组受约束输入参数的条件下最大化或最小化函数优化技术|先尝试线性规划，因为整数和非线性规划会运行较长时间。|Winston, Wayne L. Operations Research: Applications and Algorithms. Connecticut: Cengage Learning, 2003. Print.|
|MCMC采样|一种通过贝叶斯模型来估计给定数据联合分布参数的抽样方法|在解决高维度问题时，如果使用MCMC进行采样，那么棘手的问题可以变得易处理。可追溯性是对潜在关系进行统计的结果，也就是说，利用蒙特卡罗进行采样，同时考虑到马尔可夫链的随机顺序过程。|Andrieu, Christophe, Nando de Freitas, Amaud Doucet, and Michael I. Jordan. “An Introduction to MCMC for Machine Learning.” Machine Learning. Kluwer Academic Publishers, 2003. Print.|
|蒙特卡罗算法|用于生成随机数的算法|在数值积分、求解微分方程、计算贝叶斯后验概率、高维多元抽样等应用中比较有用。|Fishman, George S. Monte Carlo: Concepts, Algorithms, and Applications. New York: Springer, 2003. Print.|
|朴素贝叶斯|根据贝叶斯理论，一组特征出现的概率基于给定特征出现某种结果的概率。|该模型假设变量是相互独立的，所以它在解决变量高度依存的问题时存在问题。它能够在一次数据统计后就建立模型，所以当面对大数据集的时候能够以较少的开发时间来确定可能的模式是否存在。|Ingersoll, Grant S., Thomas S. Morton, and Andrew L. Farris. Taming Text: How to Find, Organize, and Manipulate It. New Jersey: Manning, 2013. Print.|
|人工网络|通过调整节点间的权重来学习数据中突出的特征并形成一个学习规则。|训练一个人工网络比利用现有模型去推断新的数据更加耗时。稀疏的网络连接能够帮助提告高分类性能。|Haykin, Simon O. Neural Networks and Learning Machines. New Jersey: Prentice Hall, 2008. Print.|
|离群点移除|从数据中识别及移除噪声的方法|删除离群点时要谨慎。有时，一个系统的最有趣行为就是异常数据点的存在。|Maimon, Oded, and Lior Rockach. Data Mining and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers. The Netherlands: Kluwer Academic Publishers, 2005. Print.|
|主成分分析|通过识别高度相关的维度来降维|许多大数据集包含维度之间的相关性，因此部分数据是多余的。当分析主成分时，按照方差的顺序来排序，因为方差是数据的最高级信息视角。采用skree图来推断最优的主成分的个数。|Wallisch, Pascal, Michael E. Lusignan, Marc D. Benayoun, Tanya I. Baker, Adam Seth Dickey, and Nicholas G. Hatsopoulos. Matlab for Neuroscientists. New Jersey: Prentice Hall, 2009. Print.|
|随机搜索|随机调整参数来发现一个比现有模型更好的解决方案|它被用作评判一个搜素算法好坏的一个标准。注意使用一个较好的随机数生成器和新的种子。|Bergstra J. and Bengio Y. Random Search for Hyper- Parameter Optimization, Journal of Machine Learning Research 13, 2012.|
|Lasso算法|变量选择和预测与一个可能有偏差的线性模型相结合的方法|有很多不同的方法去选择lambda参数。一个典型的选择方法是通过均方差来进行交叉验证。|Tibshirani, Robert. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological). Toronto: Royal Statistical Society, 1996. Print.|
|敏感性分析|包含分析或者模型中的检验单个参数，观察影响程度。|将不敏感的参数作为常量的候选，这样可以减少优化问题的维度，并提供一个算法加速的机会。|Saltelli, A., Marco Ratto, Terry Andres, Francesca Campolongo, Jessica Cariboni, Debora Gatelli, Michaela Saisana, and Stefano Tarantola. Global Sensitivity Analysis: the Primer. New Jersey: John Wiley & Sons, 2008. Print.|
|模拟退火|退火是冶金工业控制的冷却过程的专有名词，并通过类比，使用温度变化或退火时间表来改变算法的收敛性。|标准的“退火”功能允许开始时广泛探索参数空间，然后在更小范围内搜索。依赖于搜索的优先级，“退火”功能可以在更高温度时进行更长时间的探索性搜索。|Bertsimas, Dimitris, and John Tsitsiklis. “Simulated Annealing.” Statistical Science. 1993. Print.|
|逐步回归|一种变量选择和预测方法。将Akaike信息准则AIC作为选择度量。由此产生的预测模型是基于普通最小二乘法或基于极大似然的一般线性参数估计。|在考虑使用逐步回归时，必须要谨慎，因为过拟合经常发生。通过限制自由变量的数目来减少过度拟合的可能。|Hocking, R.R. “The Analysis and Selection of Variables in Linear Regression.” Biometrics. 1976. Print.|
|随机梯度下降|用于优化神经网络、支持向量机、逻辑回归模型|在使用子梯度时当目标函数不是绝对可微时使用。|Witten, Ian H., Eibe Frank,  and Mark A. Hall. Data Mining: Practical Machine Learning Tools and Techniques. Massachusetts: Morgan Kaufmann, 2011. Print.|
|支持向量机|衡量语料库中一个词的相对重要性的一种方法|尝试使用不同的核函数，并用k折交叉验证来验证选取的最好模型|Hsu, Chih-Wei, Chih-Chung Chang, and Chih-Jen Lin. “A Practical Guide to Support Vector Classification.” National Taiwan University Press, 2003. Print.|
|TF-IDF|衡量语料库中一个词的相对重要性的一种方法|通常用于文本挖掘。假设在一个新闻题材的语料库中，"the"这个词可能出现在多篇文章中，但是它具有很低的价值。一个比较罕见的术语，比如某个人名只出现在一个单一的文章中，那么将获得很高的tfidf分值。|Ingersoll, Grant S., Thomas S. Morton, and Andrew L. Farris. Taming Text: How to Find, Organize, and Manipulate It. New Jersey: Manning, 2013. Print.|
|LDA主题模型|通过单词共现来确定文中的潜在主题|使用词性标注除名词和动词以外的其他词。使用原始的次计数代替TF/IDF|Blei, David M., Andrew Y. Ng, and Michael I. Jordan. “Latent Dirichlet Allocation.” Journal of Machine Learning Research. 2003. Print.|
|树模型|模型的结构与树结构一致，分支表明决策。|可用于处理一个过程，或者作为一个分类器。|James, G., D. Witten, T. Hastie, and R. Tibshirani. “Tree Based Methods.” An Introduction to Statistical Learning. New York: Springer, 2013. Print.|
|T校验|检验两个组差异的假设检验。|确保符合测试假设，并在运行多个测试时观察多重比较误差|Bhattacharyya, Gouri K., and Richard A. Johnson. Statistical Concepts and Models. Wiley, 1977. Print.|
|Wrapper Methods|通过某个模型中一组特征的表现来进行降维。通过特征组合来提高模型性能|使用k折交叉验证来控制过拟合|John, George H., Ron Kohavi, and Karl Pfleger. “Irrelevant Features and the Subset Selection Problem.” Proceedings of ICML-94, 11th International Converence on Machine Learning. New Brunswick, New Jersey. 1994. Conference Presentation.|

## 战壕里的生活 - 在齐脖深的数据里航行

### 深入数据重要领域

从这些年大量架构或者重架构分析问题，数据科学家们学习和发展了大量新的解决方案。在这一部分，我们将从数据科学领域中专家们的第一手实验中列出一些重要的课题。

### 战壕里的生活 - 在齐脖深的数据里航行：深入机器学习

> 机器通过模仿人类大脑，学习能力日臻完善。

想一下十年前，计算机能够理解你说的话并做出反馈吗？最近，语音转文字质量进步到完美的精度，手机用户相当满意。在其它复杂问题中，浮现出类似的魔力。Atari，一个没有任何专业知识的机器学习算法，保持了29款电子游戏最高记录。

深度学习使这些惊人的成就成为可能，它是另一种处理机器学习问题的方式。大多数机器学习方法需要人工组织逻辑、规则来提取特征作为机器学习模型的输入。在一些领域中，例如音频、文本、图像和信号处理中，有效的特征提取工程需要相当大的人类专业知识才能达到理想的效果。深度学习避免了必要的人工编制特征，并且把特征提取、特征选取和模型合为一步。

深度学习是起源二十世纪五十年代神经网络的扩展，深度学习是基于神经网络，神经网络起源于二十世纪五十年代，启发于我们对神经元在大脑中运转的理解。最近，硬件的发展，原本是为了加快图像渲染速度，却促进了神经网络的发展。最新的图形处理单元，即 GPU，拥有 3000 多个处理单元，非常适合并行处理图像渲染的复杂矩阵运算和神经网络计算。

二十一世纪初（2000 年代末），GPU、算法进步和大数据的结合重燃了人们对神经网络的兴趣。 GPU 使计算机在更短的时间处理更大网络，巧妙的算法进步使得模型优化更加高效（巧妙演进的算法使模型处理更高效）。海量图像、视频和文本数据提供给深度学习算法充足的内容去学习。为了能够用更多的数据训练更大的网络，促使 探索一种新的神经网络结构特征，额外增加隐藏层和扩展网络广度

现在，深度学习正在从学术应用步入我们的生活。深度学习贡献于手机与智能设备上的语音转换文本技术（比如，手机与智能设备上的语音转文字）、大技术公司的图搜索、文本和语音的语言翻译，甚至是先进制药公司的新药研制。


首个National Data Science Bowl提供数据科学家们一个能够保护（利用）他们的热情，释放（发动）他们的好奇心和扩大他们在全球范围的影响力的平台（个人的平台）。竞赛呈现给参赛者超过十万张由哈特菲尔德科学中心提供的水下照片（海底照片），参赛者挑战完成一个能够使研究人员从所未有的速度和尺度去监视海洋健康的分类算法。

#### National Data Science Bowl

首个National Data Science Bowl提供数据科学家们一个能够利用他们的热情，发动他们的好奇心和扩大他们在全球范围的影响力的个人的平台。竞赛呈现给参赛者超过十万张由哈特菲尔德科学中心提供的海底照片，参赛者挑战完成一个能够使研究人员从所未有的速度和尺度去监视海洋健康的分类算法。

超过 90 天的比赛过程中，超过 1000 支队伍提交了总共接近 15000 个解决方案。大部分参赛者基于深度学习实现的，特别是卷积神经网络（CNNs）。竞赛的讨论激发了集体知识分享与合作推进了计算机视觉的进步。参赛者尝试使用新技术开发 CNNs 并且贡献开源组织创建 CNN 模型。头三名队伍，深海队、欢乐元宵节队和泊松处理队，都在解决方案里使用了 CNNs 技术，他们的结果使算法精度比最新水平高出 10%。没有他们的算法，将会花费海洋研究员超过两倍一生的时间去人工完成人类操作。所有参赛者提交的工作代表了海洋研究和数据科学界的主要进展。

Visit [www.DataScienceBowl.com](www.DataScienceBowl.com) to learn more about the first-ever National Data Science Bowl

![](http://i4.piimg.com/0e801f32ed26f0ec.png)

### 战壕里的生活 - 在齐脖深的数据里航行：特征工程

> Feature Engineering 特征工程

特征工程是分析方法中建立代表性数据的过程。它是数据科学的基础技能。没有特征工程，我们将无法通过数字化数学模型去理解和表示真实世界。特征工程是一项极具挑战的艺术。如同其他艺术，这是一项创造性的过程，它是每个数据科学家创造独特艺术的过程。它很大程度上受科学家的经验、偏好和对工程领域的理解所影响。

当面临特征工程问题时，可能先会取多种方法中的一种。一般来说，好的特征代表更丰富的领域知识。特征工程的其中一种方法是先选取该领域的基础元素开始，再继续构建更错综复杂的特征使得模型也更加丰富。这些复杂化的特征也可以由该领域的其他属性来定义，将这些特征分组或者使用数理统计方法去设计新的特征。最终结果是由机器学习算法基于这些特征向量的性能决定的。

思考垃圾邮件分类例子。这个例子的域是一组邮件，一个可能的选择是每个单词在文本中出现的次数的向量。这被称为词袋假定，单词在文本的次序是被忽略的。如果使用该特征的算法不能充分判断垃圾和非垃圾邮件，也可以加上错误拼写的计数。这个新特征使用了垃圾邮件识别领域知识：判定许多垃圾邮件拼错单词。拼错单词判定过滤也意味着确定单词的存在将自动标明邮件是否为垃圾邮件。如果这个特征还不够，还可以考虑该邮件是否写了姓名。 

特征工程代表了数据科学复杂却又关键的一环。『 学习最优特征 』工具栏变成特征工程的任务，特征工程使用于机器学习技术的自动化方法。 


#### 好和不好的模型区别在哪里？

大部分情况下，模型的输入远远比算法的选择更重要。传统方法下用根据启发和主题专业知识两步骤去寻在一组好的特征，然后由算法优化模型参数。深度学习综合这些步骤为一整体。特征工程，特征选择和模型参数估计同步完成。这样既降低了高度专业领域知识的需要，也常常能获取更好的模型。

例如，在自然世界中获得的图像，深度网络会学习初等特征，例如各种角度的线和曲线。中间层会组合初等特征成为更复杂的几何图形和模式。更高水平的层次会结合中等特征成为类似于脸或者动物形体的更复杂特征。而面向于其他类型数据，例如音频和文本，深度神经网络以相似的方式逐层精确。

#### 化学信息学搜索

在一次任务中，我的团队面临一个挑战，要开发一个化学化合物搜索引擎。化学信息学搜索的目标是在基础化学研究中预测一个分子的排列特性并提供这些特性加快数据发现的索引。这些特性可能是离散的（例如，一个分子能够治疗 X 疾病）或者是连续的（例如，一个分子能够溶于 100.21 克／毫升中）。

分子是复杂的 3D 结构，可以由通过不同差异电子域的化学键和分子结构组成的原子团表示（通常由一系列带不同电量的原子团 <又叫根，比如碳酸根、硝酸根> 和分子组成的几何结构）。这种结构是三维的并且由表面带静电的原子组成的分子。搜索这种数据是令畏惧的，而且考虑表面上类似于图的解决方法是幼稚的。

我们研发的方案是以分子指纹分析（有时候也被称作哈希或者局部敏感哈希）的前期工作为基础。 指纹分析是指通过总结众多特征来大幅度降低问题空间的降维技术（经常会相对少的关注特征的重要性）。当一个确切的方法明显不可行，我们通常使用指纹分析这类启发式途径解决。

我们方法使用的训练集中，所有分子的可测量属性都是可获得的。我们创建了一个分子结构相似性对属性影响的可能性。我们先找了每一个分子的所有 n 长度子图结构，类似自然语言分析里的单词袋方法。我们汇总每一个分子片段为指纹：”布隆过滤器计数“。

然后，我们从集合中用几个范例去创造新的特征。我们找到了训练集中的每一个成分到每一个范例的距离。将这些特征输入一个非线性回归算法而生成一个能用于非训练集数据的模型。这个方法被概念化为隐藏流形，因此一个隐藏的表层或者模型决定了分子如何展现它的特性。我们估摸着这样的模型使用了非线性回归和已知属性数据集。一旦我们掌握了相似模型，我们就能够用它去预测新分子的属性。

我们的方法是多步骤且繁琐的：生成子图，创建布隆过滤器，测算距离和调优线性回归模型。这个例子提供了多个步骤如何融入一个生成精确特征的例证。创造性地结合和建造“特征上的特征”，我们就能够创造新的更丰富和更具表达力的典型数据，从而能够更快执行和产生更好的结果。

### 战壕里的生活 - 在齐脖深的数据里航行：特征选择

> 模型就像贵客，你应该用美食招待他。

特征选择是寻找对模型具有最高价值信息特征集的过程。两个关键的方法是过滤和包装。过滤方法使用测试统计去排出多余的和没有信息价值特征。例如，过滤方法能够过滤掉与标签分类弱相关的特征。包装方法利用分类模型作为特征挑选的一部分。一个模型用特征集训练，分类精准度用于测量特征集的信息价值。一个例子是用一个特征集训练神经网络，然后来评估模型的精准度。如果模型对测试数据集评分比较高，那么该集合特征就具有高信息价值。（所有可能的特征组合都是为了试图找到最好的特征集。

这些技术中有许多权衡。过滤方法能够快速计算，因为每个特征只需要去与类标签做比较。包装方法，从另一方面说，则是通过构建模型和测量效果评估特征集。这个需要大量的模型去训练和评估（随着特征数呈现几何（指数）量级增长）。为什么每个人用包装方法呢？特征集可能比独立特征表现得更好。使用过滤方法，一个与类标签弱相关的特征将会被排除。而这些被排除的特征中，可能，某些联合起来将会比其他特征有更好的效果。

#### 癌细胞分类

在一个项目里，我们队伍挑战分类癌细胞问题。基于 72 个只有小型特征集例子中的基因画像或者概述，我们的首要目标就是分类各种不同类型的白血病。我们用混合神经网络 (ANN) 和基因算法从在上千个特征中挑出 10 个中找到标识子集）。训练ANN并用交叉验证的方法测试性能（效果）。性能（效果）测量将作为基因算法的反馈。

当特征集包含无用信息，模型的效果表现不好，则需要探索一个不同的特征集。随着多次试验，这种方法挑到了具有高精度效果的特征集。向下选择特征集不仅加快了速度和效能，也更加深入地理解可能控制系统的因素。这也使我们团队设计一个少许基因标记代替上千标记的检验测试，因此降低了检验测试的复杂度和开销。

### 战壕里的生活 - 在齐脖深的数据里航行：集成模型

> 没有任何一个人能比所有人聪明，只有一部分人比其他人聪明。

在 1906 年，弗朗西斯-高尔顿出任一项猜测公牛体重比赛的评审。高尔顿收集了 787 位参赛者的猜测值并计算平均值。出乎他的意料，平均值仅仅比公牛真实体重 1198 磅少一磅。共同的，聚集许多业余者预测比一个专家预测更精确。

高尔顿的 『人群智慧』 扩展到数据科学则是集成学习，也可以非正式地称为集成、混合或者重叠。一种集成将各个模型的预测值集合到一起成为一个新的预测值。就像人们猜测公牛的体重，数据科学模型有独特的优势和劣势（例如，由他们的设计所决定），会被基于以前经验的多种观点所影响（例如，他们已经观察了数据）。

集成克服了个人的弱点，做出比各自模型更精准的预测结果。这些模型不需要加入其他方法的主干，一种集成可能使用同样的方法应对不同参数或者比重（例如，boosting算法），特征集合（例如，随机森林算法），或者重采样（子样本集）数据（例如，bagging算法）。集成的方法可以简单到一分为二（集成的方法可以简单到两个输出的平均），也可以复杂到使用“元模型”去学习出最优结合。

集成能够利用成员的多样性来降低个体的误差。如果一个模型过度拟合数据，可以用欠拟合的模型来平衡它。如果某个子集倾斜，可以加入没有离群值的子集。如果一种方法噪声输入不稳定，可以增加另外一种更加稳定的方法。

训练中，集成常常提高模型少许百分比。这种精准度的代价是增加了复杂度。精准度与复杂度的取舍使得难以判定什么时候使用集成是合适的。一方面，集成似乎为了合适而付出高风险代价－－想想根据 MRI 图像和在传送带上判断未成熟的蓝莓。另一方面，高风险问题需要更高的审计模型实用性标准。

数据科学家必须掌握好集成可解释性和黑箱复杂度的平衡。这可不像表面上看起来这么简单！试着当自己正坐在一辆机器学习编程自驾车的驾驶员座位上。如果一个好的回归模型99.5%做出正确决定，而一个复杂的、低解释性的集成99.8%做出正确决定，你会如何挑选呢？

#### 集成模型的价值

几年前，Kaggle 图像质量预测竞赛出了一个问题：『给定成千组图像隐藏信息，预测哪些人们会打分标记为 ‘好’ 图。』，提供给参赛者大量的用户产生图片。目标是生成一个能够自动从集合中挑选出好的、令人印象深刻的图片。

整个比赛进行后，207 人提交了作品。作品中使用对数似然度量来评价精确度。前 50 队伍的分数在 0.18434 到 0.19884 之间，分数越低越好。Kaggle 数据科学家本-哈姆纳使用了结果去例证了前 50 得分平均值的集成价值。下图展示了结果：

![](http://i4.piimg.com/8c417e33feff44fc.png)

蓝色线展示了前 50 队的各自分数。橘色线展示了前 n 队的集成分数，n 的范围是从 1 到轴值。例如，集成分数在横轴（Final Team Rank）为 5是的分数是队伍 1 到队伍 5 的集成结果。如图所示，集成分数比任何单独一队的分数还低。集成模型的多样性使得相互偏差互相抵消，从而生成一个总体上更低的分数。这个在前 50 队的所有点来看都是成立的。但是，当我们提高模型数大于 15 后，我们开始看到集成分数提升了。这种现象是因为我们引入了低精确模型（例如，隐藏的过度拟合）进入集成中。这次简单试验的结果衡量了创造集成模型的价值，在挑选个体模型时进入集成时，我们必须三思后行。

### 战壕里的生活 - 在齐脖深的数据里航行：数据准确性

> 我们是数据科学家，不是数据炼金术士。我们不能从数据中分析出黄金。

当大多数人认为大数据具有体量大、产生快和种类多的特征时，还有一个同等重要却常被忽略的维度：数据准确性。数据准确性是指综合质量情况和数据正确性。你必须评价数据的真实性和精准度，还要标识出丢失和不完整数据。俗话说：“垃圾进，垃圾出”如果你的数据是不精确的或者丢失了信息，你就不会分析出黄金。

评价数据真实性往往是主观的。你必须依赖于你的经验和对数据来源和内容的上下文环境理解。领域专家的专长就是后者。尽管数据精准度评价可能也是主观的，有时也有定量的方法可以使用。可以重新采样并且统计和已存储的数据比较，从而测量出精准度。

你会遇到的最普遍情况是丢失或者信息不完整。这里有两种最基本处理丢失数据的策略－删除和填补。在前者中，全面观察分析，减少样本量，潜在地引入偏差，填补丢失或者错误的数据，可以使用多种技术，例如随机取样（就近补齐）、平均值取代，统计分布或者模型等。

#### 专家建议

找到有效的方法，实施，行动。你担心最优化，可以在递增提升中逐步改进方法。

#### 时间序列模型化

在我们的一个项目中，我要找出时间序列和各种参数的相关关系。我们最开始的分析表明了大部分之间不存在相关关系。我们验证了数据，很快发现了数据准确性问题。数据里有丢失数据和空值数据，和消极的观测值（负值），一种内容中不可能出现的测量值（看下图－清理前的数据）。垃圾数据意味着垃圾结果。 

![](http://i4.piimg.com/f84860262ead830f.png)

因为样本已经足够小了，删除观测值是不理想的。时间序列不稳定的本质意味着采样填补的数据是不可信的。最终，我们很快想到了最好的解决策略是过滤和矫正数据里的噪音。

我们最开始尝试了简化方法－通过移动平均去代替观测值。在纠正某些噪音的时候，移动时间序列包括离群点的移动平均计算。这样在底层信号中造成了不合理想的扭曲，于是我们很快放弃了这张方法。

我们队伍中的一个在信号处理有丰富经验的成员推荐了用中值滤波器。中值滤波器是移动窗口内的点，然后用中间值计算当前窗。我们试验了多种窗口大小找到一个可接受的权衡在噪音平滑和信号平滑。下图，清理后的时间序列数据，展示了中间值滤波器填补后的两种时间序列。

![](http://i4.piimg.com/7aaee8a5eb7dc935.png)

中间值滤波器的应用方法得到了巨大的成功。时间序列图可视化检验揭露了在没有抑制自然出现的峰值和谷值（数据丢失）下平滑了离群点。在平滑前，我们的数据中得不出相关关系，平滑后，大部分参数的Spearman相关系数是0.5，通过处理数据准确性问题，我们能够分析出黄金。当其他途径可能已经证明有效的时候，填补方法实施的速度限制了我们进行任何更深入的分析。之后我们取得了成功，并转移到解决问题的其他方面。

### 战壕里的生活 - 在齐脖深的数据里航行：领域知识的应用

> 我们在各自领域下都是独特的，不要贬低你所获取到的知识。

我们在各自领域下都是独特的，不要贬低你所获取到的知识。问题所依赖的领域知识对其具有非常重要的价值且无可替代。它使你对你的数据有深刻的认识，这个要素将影响你的分析目标。大多时候，领域知识是数据科学团队成功的关键区分器。领域知识影响着我们工程设计、特征选择、数据导入、算法挑选和成功与否。但是一个人不可能是所有领域的专家，我们只能在团队所掌握的领域知识的基础上，依靠其他分析师或者领域专家发表的论文或者研究结果来丰富领域知识。

#### 机动车盗窃

我曾经做过这样一个项目:怎样通过数据科学技术提高公共安全。根据 FBI 调查显示，每年大概因机动车盗窃案损失了 80 亿美元。而美国每年一百万辆机动车盗窃案破案率低于 60%。处理这些犯罪问题消耗了大量的法律资源。我们想，是否能够通过其他方式减少盗窃案，从而更加有效的利用法律资源？

我们的团队分析验证了旧金山的犯罪数据，用城市基本信息丰富汽车失窃数据，再结合时间和地域两个维度。在几次数据实验后，我们发现了几个犯罪高发的区域和时间段（如：盗窃发生时间区域热力图所示）。

团队里的领域专家发现热点区域主要分布在公园周围。这个公园建在一个市内高地上面，并且有多个进出口，这方便了盗车贼。 

![](http://i4.piimg.com/0f438e283fd55eb9.png)

我们团队协同领域专家的专业见解，总结了热力图信息，开发出了一个蒙特卡洛模型，能够凭此预测城市里摩托车盗窃交集点的相似值。根据预测得到的犯罪高发地点，有效的帮助了政府部门掌握盗贼的特征，机动车盗窃案件的发生得到了有效的控制，法律资源的利用更加充分。

领域专家的精准观察和深刻分析，使街区变得更加安全。

### 战壕里的生活 - 在齐脖深的数据里航行：维度诅咒

> 除了PCA，没有任何魔法药剂可以解开这个诅咒。

『维度诅咒』 是严重影响机器学习因素的其中之一。大部分机器学习文章会在第一、二章节中提起这种现象，但往往需要日积月累的实践后才能理解其真正内涵。

大多数的数据挖掘中的分类算法都有可能导致维度诅咒。直觉上，维度诅咒就是随着数据维度的增加，得到一个普适的分类模型（模型能够适应所有场景而不只是在训练数据中表现良好）会变得越来越难。

在现实情景下，这个障碍通常无法克服。领域（问题）中总有一些突发情况你必须最小化降低维度才能解决。这就需要结合精湛的特征工程和降维技术的使用（可翻阅 战壕生活章节中的 特征工程和特征选择）。

从我们的实践经验上来看，线性模型的最大维度数量最好控制在10几个。比较精妙的方法例如SVM（支持向量机）的维度上限应该是几万个。但是无论如何，最高维度数的限制依然存在。

维度诅咒有一个令人不可思议的结论，就是它限制了分类模型训练所需要的数据。

导致这种情况大致有两个原因：一方面，维度数已经缩减到足够小，模型可以在单机进行训练；另一方面，这个限制指数增加了高维问题的复杂度使得在实践中无法完成训练模型的计算。

在我们经验中，一个问题中很难在这两端中取得一个最佳平衡点（既足够小，又不会大到难以训练）。

在训练模型的过程中，与其试图实现一个超具扩展性的算法，还不如致力于用现有的基础算法来解决你目前的问题。

在实际操作中，直到你遇到算法无法收敛，或者交叉检验结果不够理想的时候，再考虑换一种方法，最后当你发现所有的基础算法都不能够解决问题的时候，再考虑自己创建一个新的方法。

这种工作模式的预期成本要比过度开发模式小的多。 

换句说法，“怎么简单怎么来，笨蛋。”

#### 烘培蛋糕

我曾经被要求用给定的 1600 个预测变量和 16 个目标变量的时间序列数据集去创建多个模型用以预测目标值。

客户要挑战着掌握大量变量下的相关复杂度，因此需要帮助。不仅我需要处理这例维度诅咒，而且预测变量实在是多种多样。初步感觉，就像是用橱柜里的所有东西去烤一个蛋糕。这绝逼不是一个好方式去烤蛋糕（做预测）！

数据多样性可以用时间序列预测不需要全部相同周期的数据这一事实来逐步分解。

在这个数据集中，目标时间序列是日周期的，然而预测数据却包含日周期，周周期，月周期和季度周期。当然，我可以把所有不相关维度全都删减或者置零，但这样可能并不会得到一个好的结果。

所以，我选择了神经网络去评估周变量的贡献度。用这种方法，我用一周的时间，在不用增加太多的维度的情况下，完成了对周数据的分析。

对其它目标变量的预测，我使用了包括投射法和关联法等多种技术对其进行掐头去尾工作。我的这种方法成功地降低了变量数目，完成了客户让问题空间变得易处理的目标。最终，蛋糕做好起来了。

### 战壕里的生活 - 在齐脖深的数据里航行：模型验证

> 重复唠叨听到的，并不意味着你已经学到了什么。

模型验证是构造任何模型的核心步骤。它能够说明“预测与观测数据之间的符合程度”问题。如果没有足够多的数据，模型无法做到精准，但是如果有太多的数据，又可能让模型局限在数据中。如果模型训练时太过注重数据的细节，会无法做到对训练数据外的场景进行准确预测，这叫做模型过度拟合。

已存在许多技术能够解决模型过度拟合问题。最简单的一种是将你的数据集分成训练集、测试集和验证集。训练数据用于构建模型，模型构建完后去估计测试数据，估计的结果用于降低模型误差。这样其实就是间接地在模型构建使用测试数据去降低模型过度拟合可能性。最后，用模型在验证数据的估计结果去评价其普适性。

训练数据用于构建模型，模型构建完后去估计测试数据，估计的结果用于降低模型误差。这样其实就是间接地在模型构建使用测试数据去降低模型过度拟合可能性。最后，用模型在验证数据的估计结果去评价其普适性。另外有一些方法是将数据只分成训练集和测试集，例如：k-fold cross-validation（k折交叉验证），Leave-One-Out cross validation（弃一法交叉验证），bootstrap（引导）法和resampling（重抽样）法。其中，Leave-One-Out cross-validation 方法通常能让模型得到比较理想的表现。操作步骤如下：在训练集中抽样出一部分数据作为测试集，其余数据作为训练集，模型在测试集中预测出现的误差会保存下来，然后再进行下一轮的抽样，训练，测试，不断循环一直到数据集中的所有数据都被抽样过，最后计算平均误差作为模型的误差。 

还有其它的方法用于评估你的假设和数据之间的相关性。统计学方法中通常是去计算决定系数，通常也被称作R平方值，用于确定你模型所预测的数据的准确性。

值得注意的是，随着你特征空间中维度增多，R平方值也会增加。校正后R平方值能通过对模型复杂度进行惩罚而补偿这种现象。当校验整个回归结果时，用解释方差除以非解释方差获得F校验值。如果一个回归结果有很高的F统计值和大于0.7的校正后R平方值。

## 全部放到一起 - 我们的案例研究

### 全部放到一起 - 我们的案例研究：精简药物审查

> 嘿（在以上的论述中），我们已经给了你很多非常棒的技术内容（我们已经对许多很好的技术进行了讨论分享）。我们知道这部分已经有市场材料的界面外观（市场化材料的外观和感觉），但这仍然有一个非常棒的故事（但是仍然还有一些好的案例要讲）。记住，故事讲述有多种形式和方式（案例讲述的形式多种多样），其中之一就是市场版本（其中的一种就是从市场的角度来描述）。你应该读读这个章节---用市场口吻讲述伟大的信息（你应该读一读以下的内容，因为里面蕴含了市场所反应的有用信息）。 

#### 分析挑战 

美国食品与药物管理局（Food and Drug Administration 简称 FDA）,其职能是促进社会公共卫生发展。主要负责支撑新疗法的推广，产品安全性、功效和质量的监管以及主导相关研究来驱动医药创新。虽然世界上最大的监管信息和科学数据仓库的其中之一被 FDA 囊括其中，但是评价人员却不能够方便地利用数据驱动的理念和分析方法来进行信息抽取，以及探测和发现加强监管决策保障公共健康安全的趋势和信号。另外，数据量和速率的激增以及各种各样必须被分析的数据加大了监管的挑战，这些数据在数据水平、格式以及质量上均表现出了相关差异。以上因素限制了 FDA 中从事药物改进和研究的监管科学家进行学术和产品的交叉以及在产品回归时期的元分析能力。

Booz Allen从事研究发展新兴的信息工具方法和技术，这些方法和技术则决定了FDA在药物发展和研究中面临监管挑战的能力。主要目标是确保CDER（Drug Evaluation and Research）社区能够高效利用机构所掌握的昂贵数据资源进行有效药物评估，当然这些评估方法是基于自然语言处理技术(NLP)，数据整合以及数据可视化方法等信息技术的设计发展。

##### 我们的方法

为了支撑在CDER方面可转换性的改变，我们设计和发展了一系列信息学原型，用来进行结构化、非结构化以及片段化的复杂数据集合的分析和建模。我们发展复合元型来为新兴信息工具方法和技术的发展保驾护航，确保能进行关键临界值方法的设计研究，即通过使各种各样复杂的数据信息集于一点的技术来进行模式识别，同时促进保护公共健康的新策略的发展。例如，引进NLP算法在全数据库中进行不利事件的比对以及引入地理可视化手段来支撑药物生产设备的调研调查。

- 产品安全分析

	在整个产品生存周期里对不利事件的调查、监察和分析需要重要数据资源。对可被指控现象辨识的能力需要大量工作上的投入，包括主动地或者被动地对不利时间的调查。为了应对挑战，我们开发了一款名为“产品安全仪表盘”的应用，其主要对不利事件所涉及到产品标签（比如：药品说明书？）进行对比。通过使用NLP技术，我们从产品标签中抽取不利事件来形成结构化的标签数据表而不是非结构化数据。这个仪表盘设置安全计算值来观察某个被报道的不利时间是否是已知的，但是却不用必须靠近外部数据资源和读取产品标签。

- 产品质量分析

	为了对 CDER 的调查和管理产品质量的任务形成支撑，需要提高产品质量检测的效率和功效，为此新的工具和方法是必不可少的。建立一个涉及生产、设备以及和个人设施相关联的产品画像，整合分散的数据资源是第一步。我们研究生成 『生产地清单报告』。这款地理上可视化的工具对原始数据进行处理和转化，从而通过特征映射来形成一个用户友好的可视化界面，以此来加强CDER的调查能力并且向评价人员提供确定设备数据和产品质量关系的能力。

##### 我们的影响

由于 FDA 负责美国人每一美元中 25 分的开销，所以机构充分利用监管信息并有效地整合历史矛盾数据来快速检测产品质量和安全问题的能力对于保障公共安全是至关重要的。NLP 的思路使得CDER能够更高效率地进行更广泛的文本数据的检索分析并且加强了从外部数据获得线索的能力，尽管这些信息可能看上去是不相关的。数据整合以及合理可视化可以直接提高研究人员的效率，具体做法是通过被设计用来在不同数据上揭示趋势和模式的仪表盘的深思熟虑，来减少研究人员在研究过程中经常性的合计或者计算的颗粒数量同时主动向评价人员展示需求频率高的数据集合。这些新的能力加强了FDA监管决策的能力，同时带动了个人医疗的发展，并且确保能够在早期对公众医疗安全信号进行探测发现。

### 全部放到一起 - 我们的案例研究：减少航班延误

#### 分析挑战

每年国内航线出发延误预估耗费了美国经济的 329 亿美金。美国联邦航空管理局（FAA）的交通流量管理系统（TFMS）被用于整体上进行航班管理。同时系统中包括了航班延误程度的预测引擎能够简单试探性的预测航班延迟。然而，根据其现存的延迟管理计划，这样简单的启发式的预测能力限制了 FAA 的行动能力。作为回应，FAA 的下一代先进概念和技术发展组想要建立一个预测概率模型来提高航空出发时间预测。这种新模型能够帮助 FAA 了解出发延误的起因、完善政策和提高出发时间的预测准确性以方便实时航空交通流量管理。

#### 我们的方法

商业航空工业有丰富的航班运营数据，大部分是政府网站公开获得的，少部分是从供应商定制而来。Booz Allen Hamilton 综合这些源头聚集了超过4TB的数据，内容涵盖了从 2008 年到 2012 年每一班每一班从美国航空出发的商业航班，其数据的信息量详细到航道、空域拥堵、天气情况、网络效应、交通管理倡议、飞机和航班详情等多个方面。这份数据包括了超过 5 千万个航班，并且涉及到了每个航班的 100 个变量。数据中包括从原始数据计算而来的合成变量（例如即将到来的航班延误），以此来捕捉航班飞行作业的动态过程。在具体的处理过程中，数据的获取、处理、质量控制以及完全不同的数据及之间的精确度都扮演重要角色。

团队使用监督学习算法开发了贝叶斯置信网络（BNN）模型去预测航班出发偏差。模型开发中最关键的步骤是选择离散化模型变量的最优算法，以及从数据中挑选恰当的机器学习技术模型。团队遵循信息理论原则去离散化模型变量从而最大化模型预测效率，之后用尽可能少量的网络复杂度进行数据表达。Booz Allen 基于航班出发时间将模型变量分成三段不同的种类：24 小时，11 小时和 1 小时。确定的航班变量仅在特定的预定出发时间才知道。例如，一次航班的航道和空域拥堵变量仅仅在起航前才知道，因此这些变量特征仅仅在一个小时种类里，航班延迟对于上述三个时间界限进行分别预测。

#### 我们的影响

在一个典型机场，模型递交了一份对比于之前FAA预测每年改善了十万到五十万分钟的延误预测。该模型也能使用于其他与航空利益相关的团队，例如航空公司能够更方便更高效深入地了解和预测网络航班延误。这样能够提高航空公司运行决策效率，包括可以在由于天气或者区域阻挡出现中断的情况下进行更积极的计划调整。出发预测的准确性的提高将改善FAA对机场、行业和其他资源的预测，并且有潜力去提高实时的交通流量管理，能够显著地减少航空公司由于出发延误导致的相关经济成本。这意味着建立了一个越来越成熟和高效的航空网络。

### 全部放到一起 - 我们的案例研究：让疫苗更安全

#### 分析挑战

美国食品与药物部门（FDA）负责的生物制品发展与研究（CBER）主要通过确保生物制品（比如疫苗，血液和血液制品）的安全性来保障公共卫生安全。CBER 当前的监督处理流程需要由医疗专家工作人员集中进行人工调查，这并不是一个短期的工作量，限制了评审周期的长期改善。另外，机构收到的大量的不利事件（AE）使得评审人员难以通过产品和病患群体的交叉来比较安全事件。

CBER 聘请了 Booz Allen Hamilton 来进行先进分析方法的研发，目的是为了进行 AE 报告的整理分类以及分析主要目标是：

- 采用自然语言处理技术（NLP）来减轻人力压力，采用自然语言处理(NLP)技术来减少资源压力，具体的做法是同过例如文本分类以及关键内容的抽取等半自动化技术优化人工评价的步骤和过程。
- 网络可视化技术能够一些可以选择的数据集之间的交互影响并且可以对AE的模式识别进行支撑。通过将NLP以及网络分析能力整合进医药部门的评审过程中，Booz Allen成功的为决策人员提供了重要的产品信息相关的风险指标以及可能的缓解方法来减少风险。

#### 我们的方法

我们设计和开发了一系列对AE报告数据集进行分析和可视化的原型，其中AE报告数据集包括结构化的和非结构化的原型。我们开发嵌套了 NLP 以及网络分析的工具来扩展加强 CBER 对于生物制品安全性的监控以及在产品的整个生命周期进行安全管理的能力。

##### 不利事件文本挖掘分析

行AE报告的评审需要涉及到分类过程，分类过程的进行时基于某个产品和已经报道过的恶劣事件之间相关关系的似然估计。由于许多AE报告是非结构化的文本，且大部分报告并不是直接与牵涉其中的生物制品的使用相关，因此人工调查是耗时且效率低下的。应对这样的挑战，我们使用开源工具进行AE报告的文本挖掘和自然语言处理（NLP），并且加强和整合了相关工具。其中就包括了 Python 和 R。通过提取诊断、死亡原因、发病时间以及展现相关信息并进行调查，文本挖掘工具流水化式地加强了市场调查过程的快速进行。

##### 不利事件网络分析

对 AE 和疫苗之间的关系进行可视化，能够揭示数据中的新模式和趋势，指导调查者发现安全事件。为了辅助 CBER 医疗工作人员和研究员判别确定疫苗或者结合疫苗对病人有伤害的情况，我们使用开源工具开发了 AE 网络分析工具。这种网络分析器允许使用者挑选部分 FDA 疫苗不利事件报告系统（VAERS）的数据，生成了共生矩阵、可视化网络和网络相关度量标准（介数、亲密度、度以及强度），同时与网络节点交互去获得了对产品安全事件的更深理解。

##### 其他分析方案

更近一步，Booz Allen 重构、模块化并且扩展了CBER对疯牛病的计算模拟能力，并以此为契机提高了对血液产品的变异克雅病的风险估计，开发了进行大量数据操作分析的代码来使用蒙特卡洛马尔科夫链进行流感传播分析，为分类算法在模拟染色体数据方面的应用积累大数据集规模上的分析策略，同时生成了一个 SAS 软件的 『宏”』来自动进行大量已知疫苗与剂量响应曲线生成的数据集匹配的相关潜力的比较。

#### 我们的影响

对于必须要判定是否一起被报道的恶劣事件是由某个生物制品引起的评审员来说，促进生物制品的市场调研迅速进行的新方法是至关重要的。CBER 的评审员承担着快速估计潜在安全信号的的压力，当然其主要估计方法有人工进行AE报告的评估、科学文献的研究阅读以及使用频率计算以及统计算法进行累计处理分析。Booz Allen 的支撑推动了对AE报告进行数据驱动创新式的分析发展，在其中AE报告是包括结构化与非结构化数据的。我们对已有文本挖掘工具进行了 1000 倍的提速，允许 CBER 的评价员们在若干秒内运行一个提取包含VAERS报告信息的文本挖掘算法，而不再需要若干小时了，同时，我们也通过通过文本挖掘算法和网络分析工具的应用，加快了医疗工作人员的生产效率。上述提到的这些新的成果允许 CBER 能够进行市场调研流程化、从科学数据中获取相关知识以及更加准确高效地确定公众疫苗的安全性。

### 全部放到一起 - 我们的案例研究：预测大屠杀发生的相对风险以阻止未来的暴行

#### 分析的挑战

大规模的暴行发生的概率非常小但它们却是灾难性的罪行，可以被提前阻止的。对过去暴行的研究显示，我们可以侦测到暴行将要发生的早期示警信号。如果政策的制定者对这些示警及时做出反应并且采取提前的预防措施，我们就可以拯救无数生命。尽管有这样的意识，常常发生的是我们总是由于抓不住警告信号而行动太晚，就算真的有，也只是在回应大规模暴行的威胁。

美国大屠杀纪念馆启动了一项名为“早期预警计划”，目的是评估国家级别的大屠杀在未来发生的风险。长期以来，研究的目的是希望去找到哪些模型和哪些指标可以最优地帮助预测未来可能发生的暴行，进而帮助设计和执行更加有效且有针对性的提前预防措施。通过试图理解一个国家一段时间风险的相对上升或者下降的原因和规律，这个系统会加深我们对新政策和资源对避免暴行起作用的作用点的理解以及什么样的策略是切实有效的。这些会提供给政府、宣传团体以及处于风险的社会组织更早期更可靠的警示信息，从而在大屠杀发生前有更多的机会采取相关行动。

这个项目的统计风险评估试图去建立统计和机器学习的算法来预测每个超过 50 万人口的国家在以后的 12 个月内发生大屠杀的可能性。此公开可用的系统集成了开源数据库的接口，同时提供了由大屠杀纪念馆的员工、咨询师、研究团体以及其他人员开发的分析方法的源代码。大屠杀纪念馆和博思艾伦咨询公司合力推动现存方法生效，并且在一起探索统计风险评估的新方法。 

##### 我们的方法

考虑到众包的力量，同时为了让他们参加黑客日，博思艾伦咨询公司给每个员工都安装了电话------这仅仅是这个团队对于博物馆修正和执行建议的支撑的开始。80 多个博思艾伦咨询公司的软件工程师，数据科学家和社会科学家花了一个星期六的时间去参加黑客日的活动。拥有跨学科背景的队伍花费 12 个小时确认新的数据集，建立新的机器学习的模型，创建模型架构来确保模型和交流结果可视化。

黑客日之后，博思艾伦的数据科学家和大屠杀博物馆的员工一起，创建了一个数据管理的架构，为的是自动下载、集成和转化开源的用作统计评估的开源数据集。这个可扩展的架构不用费什么力气就可以进行新数据集的集合，因此能更好地支持数据科学团体的参与。

##### 我们的影响

在 2015 年秋天公开启动的 『早期预警计划』 现在已经可以进行先进的定量和定性分析，为政府、宣传团体和有风险的社会提供关于世界范围内大屠杀发生的可能性评估。综合这个计划的统计风险模型和专家意见调查，我们创造了一个公开可用的价值巨大的信息资源，并使数据科学位于全球外交的中心。

在黑客日形成的机器学习模型，其表现可以与国家运用的经验性方法比肩，也能展现未来 2-5 年效用，同时，团队确立了构造测试验证的方法以及支撑更多稳定模型计算的数据集。这种风险评估方法，对于他们本身来说，是一个重要的科技成果。但是这个计划的启动，对于整个数据科学群体在全球外交对话中的地位意义重大。它标识着那些站在大数据前沿的人们开启了一个全新的纪元。

从黑客日学到的并且发展而来的数据管理框架，对于大屠杀纪念馆来说，代表着一个巨大的飞跃。数据聚合和转换的周期从一年两次减少到了一周一次。除了提供给群体最新的数据之外，还为研究者们减少了负担，使他们能花更多时间分析数据和发现新的或突然出现的趋势。此可扩展的框架将允许大屠杀纪念馆方便地进行新数据集的整合从而使它们相对于社区而言是可利用的或者可以辨认的，从而发挥其对身边问题的分析价值。

通过这项计划，大屠杀纪念馆的作用可以从监控正在进行的暴力行为转变成判断世界的哪个地方最有可能在 12 个月到 24 个月或者是在更远的未来发生大规模暴行，这个转变是通过对先进的定性定量分析的整合来得到的。『早期预警计划』 是一个可以支持全球外交对话的非常珍贵的预测资源。虽然项目启动的初始努力集中在机器学习和数据管理技术，但是它展示了数据科学群体在全球外交讨论的中心扮演着越来越重要的角色。

### 全部放到一起 - 我们的案例研究：预测客户的反应

#### 分析挑战

想知道客户对一个促销活动作出什么反应是非常有挑战的工作。与洲际酒店集团（IHG）一起，博思艾伦探索了一些预测方法可以预测客户对促销活动的反映，以便更好地理解并提高投资回报（ROI）。

在过去，传统的统计数据分析可以很好地分析出直销促销对购买行为的影响。当前，现代的多渠道促销经常导致数据集有更多的维度并且有时会很稀疏，这使得传统统计方法有些力不从心，因为传统方法要准确判断促销对购买决策的影响。由于多渠道促销活动日益频繁，IHG 必须要探索新的方法。特别的是，洲际酒店集团和博思艾伦通过一组忠诚计划的客户的酒店使用、入住以及客户信息来进行最近一次的促销研究。

#### 我们的方法

博思艾伦与洲际酒店集团的专家紧密合作，探索调查出成熟分析阶段的三个关键的要素：

- 描述：用原始数据挖掘，通过连接多层次的不同数据集，可以识别出客户行为方面的哪些见解或倾向？
- 发现：我们可以确定哪个控制组成员将有可能为某个促销活动而注册吗？如果是这样，我们是否可以量化其注册？
- 预测：一个酒店顾客从没收到过促销突然收到优惠价会如何反应？一个酒店顾客使用过优惠价这次没收到促销会如何反应？

促销活动是本案例研究的重点，对于这个促销，并不能像传统的统计那样对顾客的各个方面进行控制。然而，基于概率的贝叶斯信度网络（BBN）可以学习所有个人客户属性以及它们对促销的影响之间的两两关系，博思艾伦研究了这种技术，可以被用来进行每个交易客户的模型刻画而不必进行严格控制因素的相似程度。

具体来说，博思艾伦开发了BBN来预测被促销活动驱动的客户与客户的影响，随后估算了促销活动的汇总投资回报率。我们使用六种机学机器技术（支持向量机，随机森林，决策树，神经网络，线性模型和提升算法）与BBN来预测促销优惠如何影响每个客户。

#### 我们的影响

概率模型能够预测客户对促销的反应，并不依赖于昂贵的对照组。这一发现使得每个促销活动可以省几百万美元。这种分析是在业界首先应用在旅游及酒店行业，而且在通过数据驱动方法来优化个人层面市场投资回报率的其他行业中，这种需求在迅速增长。

由于即使客户之间没有确切的相似的存在，博思艾伦和洲际酒店集团的方法仍然可以估计出每个假想的客户的投资回报率，所以其在将来也有很好的应用价值。一个应用场景是降低市场“垃圾邮件”风险，因为“垃圾邮件”会使得用户大量退订并对酒店品牌产生负面影响。 

## 结束时间

### 结束时间：数据科学的未来

数据科学在高速的发展并且正在影响着我们每日生活的方方面。数据科学改变着我们交互的方式，探索着我们的世界，数据科学的算法和应用继续在发展。我们期望这个趋势会继续，因为数据科学对人类有持续的深远的影响。我们将要在这里描述一下在未来几年数据科学领域我们预测将要发生的趋势和发展。

一些数据科学算法的发展将能够将能够很好的追踪数据科学家用来表达他们的研究领域的数据结构和数据模型的演进。一个最清晰的关于这个联系的例子是部署在图数据库上的大规模图分析算法的发展（包括网络山上和语义链接数据库）。通常有一种说法是“整个世界就是一张图”，因此天然的数据结构不是一个由行和列组成的表，而是一个有节点和边组成的网络图。图分析包含了传统的机器学习的算法，但是结合了图数据。

数据科学算法的另外有一个增长的方向是地理空间时间预测分析，它可以使用在任何包含地理空间位置和时间的数据集上-这正是描绘了我们每天的生活的数据。我们预计它会广泛的部署在执法、气候变化、灾难管理、人口健康、社会政治变化等等领域。

非常明显的是更大的、更快速度以及更复杂的数据集需要更快（超快）的分析。我们预计高级的数据科学算法将会受益于量子计算机学习、内存数据操作、以及在特殊设备(例如GPU,树莓派，或者下一代移动手持超级计算机）上的机器学习的技术发展。在这些商用设备上，我们会看到更多的嵌入式机器学习（特别是深度学习）算法的发展去在数据产生时就执行一些非常时间关键的洞察。这些用例将会随着包括工业IOT以及万物都联网的涌现而变得极大的丰富。

认知机器学习逐渐出现，包括开源的或者可配置的可以利用流式实时数据的全部内容、环境以及语义信息的算法。可以利用情境的360度视图能力将使得在正确的时间、正确的地点、正确的环境采取正确的活动成为可能，这是认知分析的核心。另外一个认识认知分析的视角是给定一个物体或者群体的所有的数据以及环境，算法能够识别出你需要问你的数据的正确的问题（可能不是你通常要问的问题）。

数据科学进步的另外一个方面是跟踪一类特殊类型数据的增长，这就是非结构化数据，特别是文本。这些非结构化数据的增长是现象级的，而且比数值型数据需要更丰富的算法，因为在自然语言比数值表格包含多得多的不同的含义。新的针对非结构化数据的数据科学算法将会使用在多个方向上。自然语言生成器将会被用来将数据点转换为文本，然后能够被用来自动产生数据故事。结构数据库生成器则会将文本文档或者其他非结构化数据转化为数据点（也就是转变定性数据为机器计算的定量数据）

所有的这些技术的发展，加上其他的我们不能想象的东西，将会对新的领域产生影响。一些未来数据科学将要被使用的最热门的、最重要的领域包括：

- __网络安全__ 包括高级检测、建模、预测、以及合规分析
- __健康__ 包括基因、精细制药、人口健康、医疗保健、健康数据共享和集成、健康记录挖掘以及可穿戴设备分析
- __IoT__ 包括传感器分析、智慧数据、以及突发事件发现报警和响应
- __客户参与和体验__ 包括360度视图、游戏化、即时个性化
- __智慧X__, X=城市、高速路、汽车、物流、供应链等等
- __精细Y__ Y=制药、农业、收割、制造、定价等等
- __个性化Z__ Z=市场、广告、医疗、学习等等
- __人力资产以及组织分析__
- __社会福利__

![](http://i2.piimg.com/ed7c95de31eb7bdb.png)

### 结束时间：结束语

数据科学能力是对我们每日生活的方方面面息息相关的数据进行分析。从拜访医生、开车、购物，数据科学在静静的改变着我们与世界进行交互的方式。我们期望我们已经帮助你真正的理解了你的数据的潜力以及如何通过问正确的问题变成非凡的思考者。我们希望我们能够帮助驱动数据科学的科学和艺术的前进。最最重要的是，我们希望你带着一个新发现的对数据科学的激情和兴奋离开这段旅程。

谢谢你和我们的这一段旅程。请加入我们的对话并且让我们听到你的声音。 请将你的想法和洞见发到 [data_science@bah.com](mailto:data_science@bah.com) 或者通过在 [Github](https://github.com/booz-allen-hamilton/The-Field-Guide-to-Data-Science) 上通过 pull request 来提交。 告诉我们你知道的世界，加入我们，变成这个故事的一个作者。

### 结束时间：参考资料

> - Commonly attributed to: Nye, Bill. Reddit Ask Me Anything (AMA). July 2012. Web. Accessed 15 October 2013. SSRN: <http://www.reddit.com/r/IAmA/comments/x9pq0/iam_bill_nye_the_science_guy_ama>
- Fayyad, Usama, Gregory Piatetsky-Shapiro, and Padhraic Smyth. “From Data Mining to Knowledge Discovery in Databases.” AI Magazine 17.3 (1996): 37-54. Print.
- “Mining Data for Nuggets of Knowledge.” Knowledge@Wharton,1999. Web. Accessed 16 October 2013. SSRN: http://knowledge.wharton. upenn.edu/article/mining-data-for-nuggets-of-knowledge
- Cleveland, William S. “Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics.” International Statistical Review 69.1 (2001): 21-26. Print.
- Davenport, Thomas H., and D.J. Patil. “Data Scientist: The Sexiest Job of the 21st Century.” Harvard Business Review 90.10 (October 2012): 70–76. Print.
- Smith, David. “Statistics vs Data Science vs BI.” Revolutions, 15 May 2013. Web. Accessed 15 October 2013.SSRN: http://blog.revolutionanalytics.com/2013/05/statistics-vs-data-science-vs-bi.html
- Brynjolfsson, Erik, Lorin  M. Hitt, and  Heekyung  H. Kim. “Strength in Numbers: How Does Data-Driven Decision Making Affect Firm Performance?” Social Science Electronic Publishing, 22 April 201Web. Accessed 15 October 2013.SSRN: http://ssrn.com/abstract=1819486 or  http://dx.doi.org/10.2139/ssrn.1819486
- “The Stages of an Analytic Enterprise.” Nucleus Research. February 2012. Whitepaper.
- Barua, Anitesh, Deepa Mani, and Rajiv Mukherjee. “Measuring the Business Impacts of Effective Data.” University of Texas. Web. Accessed 15 October 2013. SSRNL: http://www.sybase.com/files/White_Papers/ EffectiveDataStudyPt1-MeasuringtheBusinessImpactsofEffectiveData-WP.pdf
- Zikopoulos, Paul, Dirk deRoos, Kirshnan Parasuraman, Thomas Deutsch, David Corrigan and James Giles. Harness the Power of Big Data: The IBM Big Data Platform. New York: McGraw Hill, 2013. Print. 281pp.
- Booz Allen Hamilton. Cloud Analytics Playbook. 2013. Web. Accessed 15 October 2013. SSRN: http://www.boozallen.com/media/file/Cloudplaybook-digital.pdf
- Conway, Drew. “The Data Science Venn Diagram.” March 2013. Web. Accessed 15 October 2013. SSRN: http://drewconway.com/ zia/2013/3/26/the-data-science-venn-diagram
- Booz Allen Hamilton. Tips for Building a Data Science Capability. 2015. Web Accessed 2 September 2015. SSRN: https://www.boozallen.com/ content/dam/boozallen/documents/2015/07/DS-Capability-Handbook.pdf
- Mnih et al. 2015. Human-level control through deep reinforcement learning. Nature. 518: 529-533.
- Torán, Jacobo. “On the Hardness of Graph Isomorphism.” SIAM Journal on Computing. 33.5 (2004): 1093-1108. Print.
- Guyon, Isabelle and Andre Elisseeff. “An Introduction to Variable and Feature Selection.” Journal of Machine Learning Research 3 (March 2003):1157-1182. Print.
- Golub T., D. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J. Mesirov, H. Coller, M. Loh, J. Downing, M. Caligiuri, C. Bloomfield, and E. Lander. “Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring.” Science. 286.5439 (1999): 531-537. Print.
- Haykin, Simon O. Neural Networks and Learning Machines. New Jersey: Prentice Hall, 2008. Print.
- De Jong, Kenneth A. Evolutionary Computation A Unified Approach. Massachusetts: MIT Press, 2002. Print.
- Yacci, Paul, Anne Haake, and Roger Gaborski. “Feature Selection of Microarray Data Using Genetic Algorithms and Artificial Neural Networks.” ANNIE 2009. St Louis, MO. 2-4 November 2009. Conference Presentation.
